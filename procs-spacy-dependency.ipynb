{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "⊕ [Linguistic Features · spaCy Usage Documentation](https://spacy.io/usage/linguistic-features#pos-tagging)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-17T16:22:39.567616Z",
     "start_time": "2019-08-17T16:22:38.894671Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple apple PROPN NNP nsubj Xxxxx True False -> [2.looking]\n",
      "is be VERB VBZ aux xx True True -> [2.looking]\n",
      "looking look VERB VBG ROOT xxxx True False -> [2.looking]\n",
      "at at ADP IN prep xx True True -> [2.looking]\n",
      "buying buy VERB VBG pcomp xxxx True False -> [3.at]\n",
      "U.K. u.k. PROPN NNP compound X.X. False False -> [6.startup]\n",
      "startup startup NOUN NN dobj xxxx True False -> [4.buying]\n",
      "for for ADP IN prep xxx True True -> [4.buying]\n",
      "$ $ SYM $ quantmod $ False False -> [10.billion]\n",
      "1 1 NUM CD compound d False False -> [10.billion]\n",
      "billion billion NUM CD pobj xxxx True False -> [7.for]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dep</th>\n",
       "      <th>ent</th>\n",
       "      <th>ent_iob</th>\n",
       "      <th>governor</th>\n",
       "      <th>index</th>\n",
       "      <th>lemma</th>\n",
       "      <th>pos</th>\n",
       "      <th>tag</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>nsubj</td>\n",
       "      <td>ORG</td>\n",
       "      <td>B</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>apple</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>NNP</td>\n",
       "      <td>Apple</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aux</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>be</td>\n",
       "      <td>VERB</td>\n",
       "      <td>VBZ</td>\n",
       "      <td>is</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ROOT</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>look</td>\n",
       "      <td>VERB</td>\n",
       "      <td>VBG</td>\n",
       "      <td>looking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>prep</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>at</td>\n",
       "      <td>ADP</td>\n",
       "      <td>IN</td>\n",
       "      <td>at</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>pcomp</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>buy</td>\n",
       "      <td>VERB</td>\n",
       "      <td>VBG</td>\n",
       "      <td>buying</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>compound</td>\n",
       "      <td>GPE</td>\n",
       "      <td>B</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>u.k.</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>NNP</td>\n",
       "      <td>U.K.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>dobj</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>startup</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>NN</td>\n",
       "      <td>startup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>prep</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>for</td>\n",
       "      <td>ADP</td>\n",
       "      <td>IN</td>\n",
       "      <td>for</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>quantmod</td>\n",
       "      <td>MONEY</td>\n",
       "      <td>B</td>\n",
       "      <td>11</td>\n",
       "      <td>8</td>\n",
       "      <td>$</td>\n",
       "      <td>SYM</td>\n",
       "      <td>$</td>\n",
       "      <td>$</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>compound</td>\n",
       "      <td>MONEY</td>\n",
       "      <td>I</td>\n",
       "      <td>11</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>NUM</td>\n",
       "      <td>CD</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>pobj</td>\n",
       "      <td>MONEY</td>\n",
       "      <td>I</td>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "      <td>billion</td>\n",
       "      <td>NUM</td>\n",
       "      <td>CD</td>\n",
       "      <td>billion</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         dep    ent ent_iob  governor  index    lemma    pos  tag     text\n",
       "0      nsubj    ORG       B         3      0    apple  PROPN  NNP    Apple\n",
       "1        aux              O         3      1       be   VERB  VBZ       is\n",
       "2       ROOT              O         0      2     look   VERB  VBG  looking\n",
       "3       prep              O         3      3       at    ADP   IN       at\n",
       "4      pcomp              O         4      4      buy   VERB  VBG   buying\n",
       "5   compound    GPE       B         7      5     u.k.  PROPN  NNP     U.K.\n",
       "6       dobj              O         5      6  startup   NOUN   NN  startup\n",
       "7       prep              O         5      7      for    ADP   IN      for\n",
       "8   quantmod  MONEY       B        11      8        $    SYM    $        $\n",
       "9   compound  MONEY       I        11      9        1    NUM   CD        1\n",
       "10      pobj  MONEY       I         8     10  billion    NUM   CD  billion"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "import pandas as pd \n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp(u'Apple is looking at buying U.K. startup for $1 billion')\n",
    "\n",
    "def doc_df(doc):\n",
    "    toks={'index':[], 'governor':[], 'ent':[], 'ent_iob':[],\n",
    "          'text':[], 'lemma':[], \n",
    "          'pos':[], 'tag':[], 'dep':[]}\n",
    "    for token in doc:\n",
    "        print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_,\n",
    "              token.shape_, token.is_alpha, token.is_stop, '->',\n",
    "              '[%d.%s]'%(token.head.i, token.head.text))\n",
    "        # toks['index'].append(token.idx)\n",
    "        toks['index'].append(token.i)\n",
    "        toks['text'].append(token.text)\n",
    "        toks['lemma'].append(token.lemma_)\n",
    "        toks['pos'].append(token.pos_)\n",
    "        toks['tag'].append(token.tag_)\n",
    "        toks['dep'].append(token.dep_)\n",
    "        \n",
    "        if token.dep_=='ROOT':\n",
    "            governor=0\n",
    "        else:\n",
    "            governor=token.head.i+1\n",
    "        toks['governor'].append(governor)\n",
    "        toks['ent'].append(token.ent_type_)\n",
    "        # IOB code of named entity tag. `1=\"I\", 2=\"O\", 3=\"B\"`. 0 means no tag is assigned.\n",
    "        # \"B\" means the token begins an entity,\n",
    "        # \"I\" means it is inside an entity, \"O\" means it is outside an entity\n",
    "        toks['ent_iob'].append(token.ent_iob_)\n",
    "\n",
    "    df = pd.DataFrame(toks) \n",
    "    return df\n",
    "\n",
    "doc_df(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-17T16:35:10.531557Z",
     "start_time": "2019-08-17T16:35:10.496151Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it -PRON- PRON PRP nsubj xx True True -> [1.is]\n",
      "is be VERB VBZ ROOT xx True True -> [1.is]\n",
      "a a DET DT det x True True -> [3.cat]\n",
      "cat cat NOUN NN attr xxx True False -> [1.is]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dep</th>\n",
       "      <th>ent</th>\n",
       "      <th>ent_iob</th>\n",
       "      <th>governor</th>\n",
       "      <th>index</th>\n",
       "      <th>lemma</th>\n",
       "      <th>pos</th>\n",
       "      <th>tag</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>nsubj</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>-PRON-</td>\n",
       "      <td>PRON</td>\n",
       "      <td>PRP</td>\n",
       "      <td>it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ROOT</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>be</td>\n",
       "      <td>VERB</td>\n",
       "      <td>VBZ</td>\n",
       "      <td>is</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>det</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>a</td>\n",
       "      <td>DET</td>\n",
       "      <td>DT</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>attr</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>cat</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>NN</td>\n",
       "      <td>cat</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     dep ent ent_iob  governor  index   lemma   pos  tag text\n",
       "0  nsubj           O         2      0  -PRON-  PRON  PRP   it\n",
       "1   ROOT           O         0      1      be  VERB  VBZ   is\n",
       "2    det           O         4      2       a   DET   DT    a\n",
       "3   attr           O         2      3     cat  NOUN   NN  cat"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp(u'it is a cat')\n",
    "doc_df(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-17T16:53:51.638840Z",
     "start_time": "2019-08-17T16:53:51.077859Z"
    }
   },
   "outputs": [],
   "source": [
    "from sagas.nlu.spacy_helper import spacy_doc\n",
    "doc = spacy_doc('hello world', 'en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-17T17:41:25.903743Z",
     "start_time": "2019-08-17T17:41:25.658359Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "verb_domains(be)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rel</th>\n",
       "      <th>index</th>\n",
       "      <th>text</th>\n",
       "      <th>lemma</th>\n",
       "      <th>children</th>\n",
       "      <th>features</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>nsubj</td>\n",
       "      <td>1</td>\n",
       "      <td>it</td>\n",
       "      <td>-PRON-</td>\n",
       "      <td>[it]</td>\n",
       "      <td>[c_pron, x_prp]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>attr</td>\n",
       "      <td>4</td>\n",
       "      <td>cat</td>\n",
       "      <td>cat</td>\n",
       "      <td>[a, cat]</td>\n",
       "      <td>[c_noun, x_nn]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     rel  index text   lemma  children         features\n",
       "0  nsubj      1   it  -PRON-      [it]  [c_pron, x_prp]\n",
       "1   attr      4  cat     cat  [a, cat]   [c_noun, x_nn]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attr -> \u001b[32ma cat\u001b[0m\n",
      "text: it \tlemma: -PRON-\tupos: PRON\txpos: PRP\n",
      "text: is \tlemma: be\tupos: VERB\txpos: VBZ\n",
      "text: a \tlemma: a\tupos: DET\txpos: DT\n",
      "text: cat \tlemma: cat\tupos: NOUN\txpos: NN\n",
      "it -> nsubj, 2, is\n",
      "is -> root, 0, _root_\n",
      "a -> det, 4, cat\n",
      "cat -> attr, 2, is\n",
      "('it', 2, 'nsubj')\n",
      "('is', 0, 'root')\n",
      "('a', 4, 'det')\n",
      "('cat', 2, 'attr')\n"
     ]
    },
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.40.1 (20161225.0304)\n",
       " -->\n",
       "<!-- Title: deps Pages: 1 -->\n",
       "<svg width=\"425pt\" height=\"98pt\"\n",
       " viewBox=\"0.00 0.00 424.94 98.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 94)\">\n",
       "<title>deps</title>\n",
       "<polygon fill=\"#ffffff\" stroke=\"transparent\" points=\"-4,4 -4,-94 420.9394,-94 420.9394,4 -4,4\"/>\n",
       "<!-- it -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>it</title>\n",
       "<polygon fill=\"none\" stroke=\"#ff0000\" stroke-dasharray=\"5,2\" points=\"282.1001,-54.0493 283.8761,-54.1479 285.6338,-54.2953 287.366,-54.4913 289.0656,-54.7353 290.7258,-55.0266 292.3401,-55.3645 293.9021,-55.7479 295.4061,-56.1759 296.8463,-56.6472 298.2176,-57.1606 299.5152,-57.7147 300.7347,-58.308 301.8725,-58.9388 302.9249,-59.6054 303.8892,-60.3059 304.763,-61.0385 305.5444,-61.8012 306.2321,-62.5918 306.8251,-63.4082 307.323,-64.2481 307.7261,-65.1093 308.0347,-65.9894 308.25,-66.886 308.3733,-67.7965 308.4065,-68.7186 308.3517,-69.6497 308.2115,-70.5873 307.9886,-71.5287 307.6862,-72.4713 307.3075,-73.4127 306.8561,-74.3503 306.3357,-75.2814 305.7499,-76.2035 305.1028,-77.114 304.3982,-78.0106 303.6401,-78.8907 302.8323,-79.7519 301.9788,-80.5918 301.0834,-81.4082 300.1498,-82.1988 299.1815,-82.9615 298.1819,-83.6941 297.1544,-84.3946 296.1019,-85.0612 295.0274,-85.692 293.9336,-86.2853 292.8229,-86.8394 291.6976,-87.3528 290.5598,-87.8241 289.4114,-88.2521 288.2541,-88.6355 287.0893,-88.9734 285.9184,-89.2647 284.7426,-89.5087 283.563,-89.7047 282.3805,-89.8521 281.1961,-89.9507 280.0105,-90 278.8245,-90 277.6389,-89.9507 276.4545,-89.8521 275.272,-89.7047 274.0924,-89.5087 272.9167,-89.2647 271.7458,-88.9734 270.581,-88.6355 269.4236,-88.2521 268.2752,-87.8241 267.1374,-87.3528 266.0121,-86.8394 264.9015,-86.2853 263.8076,-85.692 262.7331,-85.0612 261.6807,-84.3946 260.6531,-83.6941 259.6536,-82.9615 258.6853,-82.1988 257.7516,-81.4082 256.8562,-80.5918 256.0027,-79.7519 255.195,-78.8907 254.4368,-78.0106 253.7322,-77.114 253.0851,-76.2035 252.4994,-75.2814 251.9789,-74.3503 251.5275,-73.4127 251.1489,-72.4713 250.8464,-71.5287 250.6236,-70.5873 250.4833,-69.6497 250.4286,-68.7186 250.4617,-67.7965 250.5851,-66.886 250.8003,-65.9894 251.109,-65.1093 251.512,-64.2481 252.01,-63.4082 252.603,-62.5918 253.2906,-61.8012 254.072,-61.0385 254.9458,-60.3059 255.9101,-59.6054 256.9626,-58.9388 258.1003,-58.308 259.3199,-57.7147 260.6175,-57.1606 261.9888,-56.6472 263.429,-56.1759 264.9329,-55.7479 266.495,-55.3645 268.1093,-55.0266 269.7695,-54.7353 271.4691,-54.4913 273.2012,-54.2953 274.9589,-54.1479 276.735,-54.0493 278.5221,-54 280.3129,-54 282.1001,-54.0493\"/>\n",
       "<text text-anchor=\"middle\" x=\"279.4175\" y=\"-67.8\" font-family=\"Calibri\" font-size=\"14.00\" fill=\"#000000\">it</text>\n",
       "</g>\n",
       "<!-- is -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>is</title>\n",
       "<polygon fill=\"#d3d3d3\" stroke=\"#d3d3d3\" points=\"162.3163,-27.0493 164.0924,-27.1479 165.8501,-27.2953 167.5822,-27.4913 169.2818,-27.7353 170.942,-28.0266 172.5563,-28.3645 174.1184,-28.7479 175.6223,-29.1759 177.0625,-29.6472 178.4338,-30.1606 179.7314,-30.7147 180.951,-31.308 182.0887,-31.9388 183.1412,-32.6054 184.1055,-33.3059 184.9793,-34.0385 185.7607,-34.8012 186.4483,-35.5918 187.0413,-36.4082 187.5393,-37.2481 187.9423,-38.1093 188.251,-38.9894 188.4662,-39.886 188.5895,-40.7965 188.6227,-41.7186 188.5679,-42.6497 188.4277,-43.5873 188.2048,-44.5287 187.9024,-45.4713 187.5238,-46.4127 187.0724,-47.3503 186.5519,-48.2814 185.9662,-49.2035 185.319,-50.114 184.6144,-51.0106 183.8563,-51.8907 183.0486,-52.7519 182.1951,-53.5918 181.2997,-54.4082 180.366,-55.1988 179.3977,-55.9615 178.3981,-56.6941 177.3706,-57.3946 176.3181,-58.0612 175.2437,-58.692 174.1498,-59.2853 173.0391,-59.8394 171.9139,-60.3528 170.7761,-60.8241 169.6277,-61.2521 168.4703,-61.6355 167.3055,-61.9734 166.1346,-62.2647 164.9588,-62.5087 163.7792,-62.7047 162.5968,-62.8521 161.4123,-62.9507 160.2268,-63 159.0408,-63 157.8552,-62.9507 156.6707,-62.8521 155.4883,-62.7047 154.3087,-62.5087 153.1329,-62.2647 151.962,-61.9734 150.7972,-61.6355 149.6399,-61.2521 148.4914,-60.8241 147.3537,-60.3528 146.2284,-59.8394 145.1177,-59.2853 144.0239,-58.692 142.9494,-58.0612 141.8969,-57.3946 140.8694,-56.6941 139.8698,-55.9615 138.9015,-55.1988 137.9679,-54.4082 137.0724,-53.5918 136.219,-52.7519 135.4112,-51.8907 134.6531,-51.0106 133.9485,-50.114 133.3013,-49.2035 132.7156,-48.2814 132.1952,-47.3503 131.7438,-46.4127 131.3651,-45.4713 131.0627,-44.5287 130.8398,-43.5873 130.6996,-42.6497 130.6448,-41.7186 130.678,-40.7965 130.8013,-39.886 131.0166,-38.9894 131.3252,-38.1093 131.7283,-37.2481 132.2262,-36.4082 132.8192,-35.5918 133.5068,-34.8012 134.2882,-34.0385 135.1621,-33.3059 136.1264,-32.6054 137.1788,-31.9388 138.3165,-31.308 139.5361,-30.7147 140.8337,-30.1606 142.205,-29.6472 143.6452,-29.1759 145.1491,-28.7479 146.7112,-28.3645 148.3255,-28.0266 149.9857,-27.7353 151.6853,-27.4913 153.4175,-27.2953 155.1752,-27.1479 156.9512,-27.0493 158.7384,-27 160.5292,-27 162.3163,-27.0493\"/>\n",
       "<text text-anchor=\"middle\" x=\"159.6338\" y=\"-40.8\" font-family=\"Calibri\" font-size=\"14.00\" fill=\"#000000\">is</text>\n",
       "</g>\n",
       "<!-- is&#45;&gt;it -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>is&#45;&gt;it</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M185.0368,-50.726C201.2612,-54.3831 222.6078,-59.1947 240.8953,-63.3168\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"240.1634,-66.7396 250.6882,-65.5242 241.7026,-59.9109 240.1634,-66.7396\"/>\n",
       "<text text-anchor=\"middle\" x=\"219.5256\" y=\"-62.2\" font-family=\"Calibri\" font-size=\"11.00\" fill=\"#000000\">nsubj</text>\n",
       "</g>\n",
       "<!-- cat -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>cat</title>\n",
       "<polygon fill=\"none\" stroke=\"#0000ff\" points=\"282.1001,-.0493 283.8761,-.1479 285.6338,-.2953 287.366,-.4913 289.0656,-.7353 290.7258,-1.0266 292.3401,-1.3645 293.9021,-1.7479 295.4061,-2.1759 296.8463,-2.6472 298.2176,-3.1606 299.5152,-3.7147 300.7347,-4.308 301.8725,-4.9388 302.9249,-5.6054 303.8892,-6.3059 304.763,-7.0385 305.5444,-7.8012 306.2321,-8.5918 306.8251,-9.4082 307.323,-10.2481 307.7261,-11.1093 308.0347,-11.9894 308.25,-12.886 308.3733,-13.7965 308.4065,-14.7186 308.3517,-15.6497 308.2115,-16.5873 307.9886,-17.5287 307.6862,-18.4713 307.3075,-19.4127 306.8561,-20.3503 306.3357,-21.2814 305.7499,-22.2035 305.1028,-23.114 304.3982,-24.0106 303.6401,-24.8907 302.8323,-25.7519 301.9788,-26.5918 301.0834,-27.4082 300.1498,-28.1988 299.1815,-28.9615 298.1819,-29.6941 297.1544,-30.3946 296.1019,-31.0612 295.0274,-31.692 293.9336,-32.2853 292.8229,-32.8394 291.6976,-33.3528 290.5598,-33.8241 289.4114,-34.2521 288.2541,-34.6355 287.0893,-34.9734 285.9184,-35.2647 284.7426,-35.5087 283.563,-35.7047 282.3805,-35.8521 281.1961,-35.9507 280.0105,-36 278.8245,-36 277.6389,-35.9507 276.4545,-35.8521 275.272,-35.7047 274.0924,-35.5087 272.9167,-35.2647 271.7458,-34.9734 270.581,-34.6355 269.4236,-34.2521 268.2752,-33.8241 267.1374,-33.3528 266.0121,-32.8394 264.9015,-32.2853 263.8076,-31.692 262.7331,-31.0612 261.6807,-30.3946 260.6531,-29.6941 259.6536,-28.9615 258.6853,-28.1988 257.7516,-27.4082 256.8562,-26.5918 256.0027,-25.7519 255.195,-24.8907 254.4368,-24.0106 253.7322,-23.114 253.0851,-22.2035 252.4994,-21.2814 251.9789,-20.3503 251.5275,-19.4127 251.1489,-18.4713 250.8464,-17.5287 250.6236,-16.5873 250.4833,-15.6497 250.4286,-14.7186 250.4617,-13.7965 250.5851,-12.886 250.8003,-11.9894 251.109,-11.1093 251.512,-10.2481 252.01,-9.4082 252.603,-8.5918 253.2906,-7.8012 254.072,-7.0385 254.9458,-6.3059 255.9101,-5.6054 256.9626,-4.9388 258.1003,-4.308 259.3199,-3.7147 260.6175,-3.1606 261.9888,-2.6472 263.429,-2.1759 264.9329,-1.7479 266.495,-1.3645 268.1093,-1.0266 269.7695,-.7353 271.4691,-.4913 273.2012,-.2953 274.9589,-.1479 276.735,-.0493 278.5221,0 280.3129,0 282.1001,-.0493\"/>\n",
       "<text text-anchor=\"middle\" x=\"279.4175\" y=\"-13.8\" font-family=\"Calibri\" font-size=\"14.00\" fill=\"#000000\">cat</text>\n",
       "</g>\n",
       "<!-- is&#45;&gt;cat -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>is&#45;&gt;cat</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M188.325,-38.5328C204.9879,-34.7769 226.1213,-30.0133 243.8368,-26.0201\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"244.8271,-29.3848 253.8128,-23.7715 243.2879,-22.5561 244.8271,-29.3848\"/>\n",
       "<text text-anchor=\"middle\" x=\"219.5256\" y=\"-36.2\" font-family=\"Calibri\" font-size=\"11.00\" fill=\"#000000\">attr</text>\n",
       "</g>\n",
       "<!-- a -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>a</title>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"390.8785,-.0493 392.6545,-.1479 394.4122,-.2953 396.1444,-.4913 397.8439,-.7353 399.5042,-1.0266 401.1185,-1.3645 402.6805,-1.7479 404.1845,-2.1759 405.6247,-2.6472 406.996,-3.1606 408.2936,-3.7147 409.5131,-4.308 410.6508,-4.9388 411.7033,-5.6054 412.6676,-6.3059 413.5414,-7.0385 414.3228,-7.8012 415.0105,-8.5918 415.6035,-9.4082 416.1014,-10.2481 416.5044,-11.1093 416.8131,-11.9894 417.0284,-12.886 417.1517,-13.7965 417.1849,-14.7186 417.1301,-15.6497 416.9899,-16.5873 416.767,-17.5287 416.4646,-18.4713 416.0859,-19.4127 415.6345,-20.3503 415.1141,-21.2814 414.5283,-22.2035 413.8812,-23.114 413.1766,-24.0106 412.4185,-24.8907 411.6107,-25.7519 410.7572,-26.5918 409.8618,-27.4082 408.9282,-28.1988 407.9599,-28.9615 406.9603,-29.6941 405.9328,-30.3946 404.8803,-31.0612 403.8058,-31.692 402.712,-32.2853 401.6013,-32.8394 400.476,-33.3528 399.3382,-33.8241 398.1898,-34.2521 397.0325,-34.6355 395.8677,-34.9734 394.6968,-35.2647 393.521,-35.5087 392.3414,-35.7047 391.1589,-35.8521 389.9745,-35.9507 388.7889,-36 387.6029,-36 386.4173,-35.9507 385.2329,-35.8521 384.0504,-35.7047 382.8708,-35.5087 381.6951,-35.2647 380.5242,-34.9734 379.3594,-34.6355 378.202,-34.2521 377.0536,-33.8241 375.9158,-33.3528 374.7905,-32.8394 373.6798,-32.2853 372.586,-31.692 371.5115,-31.0612 370.4591,-30.3946 369.4315,-29.6941 368.432,-28.9615 367.4637,-28.1988 366.53,-27.4082 365.6346,-26.5918 364.7811,-25.7519 363.9734,-24.8907 363.2152,-24.0106 362.5106,-23.114 361.8635,-22.2035 361.2778,-21.2814 360.7573,-20.3503 360.3059,-19.4127 359.9273,-18.4713 359.6248,-17.5287 359.402,-16.5873 359.2617,-15.6497 359.2069,-14.7186 359.2401,-13.7965 359.3635,-12.886 359.5787,-11.9894 359.8874,-11.1093 360.2904,-10.2481 360.7884,-9.4082 361.3814,-8.5918 362.069,-7.8012 362.8504,-7.0385 363.7242,-6.3059 364.6885,-5.6054 365.741,-4.9388 366.8787,-4.308 368.0983,-3.7147 369.3959,-3.1606 370.7672,-2.6472 372.2074,-2.1759 373.7113,-1.7479 375.2734,-1.3645 376.8877,-1.0266 378.5479,-.7353 380.2475,-.4913 381.9796,-.2953 383.7373,-.1479 385.5134,-.0493 387.3005,0 389.0913,0 390.8785,-.0493\"/>\n",
       "<text text-anchor=\"middle\" x=\"388.1959\" y=\"-13.8\" font-family=\"Calibri\" font-size=\"14.00\" fill=\"#000000\">a</text>\n",
       "</g>\n",
       "<!-- cat&#45;&gt;a -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>cat&#45;&gt;a</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M307.9978,-18C320.6384,-18 335.7003,-18 349.3397,-18\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"349.4289,-21.5001 359.4289,-18 349.4288,-14.5001 349.4289,-21.5001\"/>\n",
       "<text text-anchor=\"middle\" x=\"333.8067\" y=\"-20.2\" font-family=\"Calibri\" font-size=\"11.00\" fill=\"#000000\">det</text>\n",
       "</g>\n",
       "<!-- ROOT -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>ROOT</title>\n",
       "<polygon fill=\"none\" stroke=\"#0000ff\" points=\"41.4783,-27.0493 43.8025,-27.1479 46.1027,-27.2953 48.3694,-27.4913 50.5936,-27.7353 52.7662,-28.0266 54.8787,-28.3645 56.9228,-28.7479 58.8909,-29.1759 60.7756,-29.6472 62.5701,-30.1606 64.2682,-30.7147 65.8642,-31.308 67.353,-31.9388 68.7303,-32.6054 69.9922,-33.3059 71.1357,-34.0385 72.1583,-34.8012 73.0581,-35.5918 73.8341,-36.4082 74.4858,-37.2481 75.0132,-38.1093 75.4171,-38.9894 75.6988,-39.886 75.8602,-40.7965 75.9036,-41.7186 75.8319,-42.6497 75.6484,-43.5873 75.3568,-44.5287 74.961,-45.4713 74.4655,-46.4127 73.8748,-47.3503 73.1937,-48.2814 72.4272,-49.2035 71.5803,-50.114 70.6583,-51.0106 69.6661,-51.8907 68.6091,-52.7519 67.4922,-53.5918 66.3205,-54.4082 65.0987,-55.1988 63.8315,-55.9615 62.5235,-56.6941 61.1788,-57.3946 59.8015,-58.0612 58.3954,-58.692 56.964,-59.2853 55.5105,-59.8394 54.0379,-60.3528 52.549,-60.8241 51.0462,-61.2521 49.5316,-61.6355 48.0073,-61.9734 46.4751,-62.2647 44.9364,-62.5087 43.3928,-62.7047 41.8454,-62.8521 40.2954,-62.9507 38.7439,-63 37.1919,-63 35.6404,-62.9507 34.0904,-62.8521 32.543,-62.7047 30.9993,-62.5087 29.4607,-62.2647 27.9284,-61.9734 26.4041,-61.6355 24.8896,-61.2521 23.3867,-60.8241 21.8978,-60.3528 20.4252,-59.8394 18.9717,-59.2853 17.5403,-58.692 16.1342,-58.0612 14.757,-57.3946 13.4123,-56.6941 12.1042,-55.9615 10.8371,-55.1988 9.6153,-54.4082 8.4435,-53.5918 7.3266,-52.7519 6.2696,-51.8907 5.2775,-51.0106 4.3554,-50.114 3.5085,-49.2035 2.742,-48.2814 2.061,-47.3503 1.4702,-46.4127 .9747,-45.4713 .579,-44.5287 .2873,-43.5873 .1038,-42.6497 .0321,-41.7186 .0755,-40.7965 .2369,-39.886 .5186,-38.9894 .9225,-38.1093 1.45,-37.2481 2.1016,-36.4082 2.8776,-35.5918 3.7775,-34.8012 4.8,-34.0385 5.9435,-33.3059 7.2055,-32.6054 8.5827,-31.9388 10.0716,-31.308 11.6675,-30.7147 13.3656,-30.1606 15.1601,-29.6472 17.0448,-29.1759 19.0129,-28.7479 21.0571,-28.3645 23.1696,-28.0266 25.3422,-27.7353 27.5663,-27.4913 29.8331,-27.2953 32.1332,-27.1479 34.4574,-27.0493 36.7961,-27 39.1396,-27 41.4783,-27.0493\"/>\n",
       "<text text-anchor=\"middle\" x=\"37.9679\" y=\"-40.8\" font-family=\"Calibri\" font-size=\"14.00\" fill=\"#000000\">ROOT</text>\n",
       "</g>\n",
       "<!-- ROOT&#45;&gt;is -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>ROOT&#45;&gt;is</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M75.4644,-45C89.8475,-45 106.2821,-45 120.789,-45\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"120.9547,-48.5001 130.9547,-45 120.9547,-41.5001 120.9547,-48.5001\"/>\n",
       "<text text-anchor=\"middle\" x=\"103.413\" y=\"-47.2\" font-family=\"Calibri\" font-size=\"11.00\" fill=\"#000000\">root</text>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.dot.Digraph at 0x12806be48>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sagas.nlu.uni_intf import WordIntf, SentenceIntf\n",
    "from sagas.nlu.corenlp_parser import get_chunks\n",
    "import sagas\n",
    "from sagas.tool.misc import print_stem_chunks\n",
    "from sagas.nlu.uni_viz import EnhancedViz\n",
    "\n",
    "class RootWordImpl(WordIntf):\n",
    "    def setup(self, token):\n",
    "        features = {'index':0, 'text':'ROOT', 'lemma':'root', 'upos':'', 'xpos':'',\n",
    "                    'feats':[], 'governor':0, 'dependency_relation':''}\n",
    "        return features\n",
    "class SpacyWordImpl(WordIntf):\n",
    "    def setup(self, token):\n",
    "        if token.dep_=='ROOT':\n",
    "            governor=0\n",
    "        else:\n",
    "            governor=token.head.i+1\n",
    "        idx=token.i+1  # start from 1\n",
    "        features = {'index':idx, 'text':token.text, 'lemma':token.lemma_, \n",
    "                    'upos':token.pos_, 'xpos':token.tag_,\n",
    "                    'feats':[], 'governor':governor, 'dependency_relation':token.dep_.lower(),\n",
    "                    'entity':[token.ent_type_, token.ent_iob_]\n",
    "                   }\n",
    "        return features\n",
    "\n",
    "class SpacySentImpl(SentenceIntf):\n",
    "    def setup(self, sent):\n",
    "        words = []\n",
    "        for word in sent:\n",
    "            words.append(SpacyWordImpl(word))\n",
    "        deps = []        \n",
    "        return words, deps\n",
    "    \n",
    "    def build_dependencies(self):\n",
    "        for word in self.words:\n",
    "            if word.governor == 0:\n",
    "                # make a word for the ROOT\n",
    "                governor = RootWordImpl(None)\n",
    "            else:\n",
    "                # id is index in words list + 1\n",
    "                governor = self.words[word.governor-1]\n",
    "            self.dependencies.append((governor, word.dependency_relation, word))\n",
    "\n",
    "class SpacyParserImpl(object):\n",
    "    def __init__(self, lang):\n",
    "        self.lang = lang\n",
    "\n",
    "    def __call__(self, sents):\n",
    "        from sagas.nlu.spacy_helper import spacy_doc\n",
    "        doc = spacy_doc(sents, self.lang)\n",
    "        return SpacySentImpl(doc)\n",
    "\n",
    "p=SpacyParserImpl('en')\n",
    "doc=p('it is a cat')\n",
    "# print(doc.words)\n",
    "doc.build_dependencies()\n",
    "# print(doc.dependencies)\n",
    "rs = get_chunks(doc)\n",
    "# print(rs)\n",
    "for r in rs:\n",
    "    df = sagas.to_df(r['domains'], ['rel', 'index', 'text', 'lemma', 'children', 'features'])\n",
    "    print('%s(%s)' % (r['type'], r['lemma']))\n",
    "    # sagas.print_df(df)\n",
    "    display(df)\n",
    "    print_stem_chunks(r)\n",
    "    \n",
    "cv = EnhancedViz(shape='egg', size='8,5', fontsize=20)\n",
    "cv.analyse_doc(doc, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Noun chunks\n",
    "Noun chunks are “base noun phrases” – flat phrases that have a noun as their head. You can think of noun chunks as a noun plus the words describing the noun – for example, “the lavish green grass” or “the world’s largest tech fund”. To get the noun chunks in a document, simply iterate over Doc.noun_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-25T01:09:01.364109Z",
     "start_time": "2019-05-25T01:09:01.305280Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autonomous cars cars nsubj shift\n",
      "insurance liability liability dobj shift\n",
      "manufacturers manufacturers pobj toward\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>head</th>\n",
       "      <th>root_dep</th>\n",
       "      <th>root_text</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>shift</td>\n",
       "      <td>nsubj</td>\n",
       "      <td>cars</td>\n",
       "      <td>Autonomous cars</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>shift</td>\n",
       "      <td>dobj</td>\n",
       "      <td>liability</td>\n",
       "      <td>insurance liability</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>toward</td>\n",
       "      <td>pobj</td>\n",
       "      <td>manufacturers</td>\n",
       "      <td>manufacturers</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     head root_dep      root_text                 text\n",
       "0   shift    nsubj           cars      Autonomous cars\n",
       "1   shift     dobj      liability  insurance liability\n",
       "2  toward     pobj  manufacturers        manufacturers"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp(u\"Autonomous cars shift insurance liability toward manufacturers\")\n",
    "\n",
    "toks={'text':[], 'root_text':[], 'root_dep':[], 'head':[]}\n",
    "for chunk in doc.noun_chunks:\n",
    "    print(chunk.text, chunk.root.text, chunk.root.dep_,\n",
    "            chunk.root.head.text)\n",
    "    toks['text'].append(chunk.text)\n",
    "    toks['root_text'].append(chunk.root.text)\n",
    "    toks['root_dep'].append(chunk.root.dep_)\n",
    "    toks['head'].append(chunk.root.head.text)\n",
    "df = pd.DataFrame(toks) \n",
    "df    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Navigating the parse tree\n",
    "spaCy uses the terms head and child to describe the words connected by a single arc in the dependency tree. The term dep is used for the arc label, which describes the type of syntactic relation that connects the child to the head. As with other attributes, the value of .dep is a hash value. You can get the string value with .dep_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-17T09:40:16.261381Z",
     "start_time": "2019-08-17T09:40:16.232873Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autonomous amod cars NOUN []\n",
      "cars nsubj shift VERB [Autonomous]\n",
      "shift ROOT shift VERB [cars, liability, toward]\n",
      "insurance compound liability NOUN []\n",
      "liability dobj shift VERB [insurance]\n",
      "toward prep shift VERB [manufacturers]\n",
      "manufacturers pobj toward ADP []\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(u\"Autonomous cars shift insurance liability toward manufacturers\")\n",
    "for token in doc:\n",
    "    print(token.text, token.dep_, token.head.text, token.head.pos_,\n",
    "            [child for child in token.children])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the syntactic relations form a tree, every word has exactly one head. You can therefore iterate over the arcs in the tree by iterating over the words in the sentence. This is usually the best way to match an arc of interest — from below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-25T01:30:54.937847Z",
     "start_time": "2019-05-25T01:30:54.275110Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{shift}\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.symbols import nsubj, VERB\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(u\"Autonomous cars shift insurance liability toward manufacturers\")\n",
    "\n",
    "# Finding a verb with a subject from below — good\n",
    "verbs = set()\n",
    "for possible_subject in doc:\n",
    "    if possible_subject.dep == nsubj and possible_subject.head.pos == VERB:\n",
    "        verbs.add(possible_subject.head)\n",
    "print(verbs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-25T01:20:48.908228Z",
     "start_time": "2019-05-25T01:20:48.211038Z"
    }
   },
   "outputs": [],
   "source": [
    "from spacy import displacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(u\"Autonomous cars shift insurance liability toward manufacturers\")\n",
    "# Since this is an interactive Jupyter environment, we can use displacy.render here\n",
    "# displacy.render(doc, style='dep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-25T01:20:56.918870Z",
     "start_time": "2019-05-25T01:20:56.229571Z"
    }
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "\n",
    "text = \"\"\"But Google is starting from behind. The company made a late push\n",
    "into hardware, and Apple’s Siri, available on iPhones, and Amazon’s Alexa\n",
    "software, which runs on its Echo and Dot devices, have clear leads in\n",
    "consumer adoption.\"\"\"\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(text)\n",
    "# displacy.serve(doc, style=\"ent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-31T08:09:20.853250Z",
     "start_time": "2019-05-31T08:09:19.745478Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36763\n"
     ]
    }
   ],
   "source": [
    "import resources_pb2 as res\n",
    "import protobuf_utils\n",
    "\n",
    "input_file='./data/langs/jpn_eng_spacy.data'\n",
    "load_langs = res.RsLangs()\n",
    "protobuf_utils.read_proto(load_langs, input_file)\n",
    "print(len(load_langs.langs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-31T08:09:08.285905Z",
     "start_time": "2019-05-31T08:09:07.049830Z"
    }
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-31T08:09:29.795803Z",
     "start_time": "2019-05-31T08:09:29.713122Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Choose a password that is easy to remember but difficult to guess. ['is'] ['be']\n",
      "Choose any dress you like. ['like'] ['like']\n",
      "Choose any one from among these. [] []\n",
      "Choose friends you can rely on. ['rely'] ['rely']\n",
      "Choose one person. [] []\n",
      "Choose one. [] []\n",
      "Choose the one you like. ['like'] ['like']\n",
      "Choose whichever you like. ['like'] ['like']\n",
      "Choose whichever you want. ['want'] ['want']\n",
      "Christianity and Islam are two different religions. ['are'] ['be']\n",
      "Christmas Day is December 25th. ['is'] ['be']\n",
      "Christmas comes but once a year. ['comes'] ['come']\n",
      "Christmas fell on Saturday that year. ['fell'] ['fall']\n",
      "Christmas is December 25th. ['is'] ['be']\n",
      "Christmas is coming soon. ['coming'] ['come']\n",
      "Christmas is coming. ['coming'] ['come']\n",
      "Christmas is fast approaching. ['is'] ['be']\n",
      "Christmas is just around the corner. ['is'] ['be']\n",
      "Christmas is just two weeks from now. ['is'] ['be']\n",
      "Christmas is near at hand, isn't it? ['is', 'is'] ['be', 'be']\n",
      "Christmas is only two weeks off. ['is'] ['be']\n",
      "Christmas is soon, isn't it? ['is', 'is'] ['be', 'be']\n",
      "Christmas is soon, right? ['is'] ['be']\n",
      "Christmas is soon. ['is'] ['be']\n",
      "Cities are exciting places, but also stressful. ['are'] ['be']\n",
      "Citizens are debating about health care at City Hall. ['debating'] ['debate']\n",
      "Class doesn't begin until eight-thirty. ['begin'] ['begin']\n",
      "Classes are starting again soon. ['starting'] ['start']\n",
      "Classes start at nine o'clock every day. ['start'] ['start']\n",
      "Classical music is not my cup of tea. ['is'] ['be']\n",
      "Clean the room. [] []\n",
      "Clean the window with a damp cloth. [] []\n",
      "Clean up the room. [] []\n",
      "Clean up your room. [] []\n",
      "Clean your room. [] []\n",
      "Close the book. [] []\n",
      "Close the door after you when you leave the room. ['leave'] ['leave']\n",
      "Close the door, please. [] []\n",
      "Close the door. [] []\n",
      "Close the drawer. [] []\n",
      "Close the gate. [] []\n",
      "Close the window. [] []\n",
      "Close your book. [] []\n",
      "Close your books. [] []\n",
      "Close your eyes, please. [] []\n",
      "Close your eyes. [] []\n",
      "Cockroaches are insects. ['are'] ['be']\n",
      "Coffee keeps me awake. ['keeps'] ['keep']\n",
      "Coffee, please. [] []\n",
      "Coincidentally enough, I know him. ['know'] ['know']\n",
      "Cold winds blow hard every winter. ['blow'] ['blow']\n",
      "Cold-war tension has mounted. ['mounted'] ['mount']\n",
      "Colds are contagious. ['are'] ['be']\n",
      "Colds are prevalent this winter. ['are'] ['be']\n",
      "Columbus discovered America in 1492. ['discovered'] ['discover']\n",
      "Comb your hair before you go out. ['go'] ['go']\n",
      "Come a bit closer. [] []\n",
      "Come again tomorrow. [] []\n",
      "Come again. [] []\n",
      "Come along with me. [] []\n",
      "Come along with us. [] []\n",
      "Come along. [] []\n",
      "Come and have tea with me. [] []\n",
      "Come and help me. [] []\n",
      "Come and help us. [] []\n",
      "Come and see me any time you like. ['like'] ['like']\n",
      "Come and see me at eleven o'clock. [] []\n",
      "Come and see me on Sunday next week. [] []\n",
      "Come and see me once in a while. [] []\n",
      "Come and see me right now. [] []\n",
      "Come and see me the day after tomorrow. [] []\n",
      "Come and see me when you have time. ['have'] ['have']\n",
      "Come and see me whenever you want to. ['want'] ['want']\n",
      "Come and see me. [] []\n",
      "Come and see this. [] []\n",
      "Come and see. [] []\n",
      "Come and sit by me. [] []\n",
      "Come and visit us in Paris sometime soon. [] []\n",
      "Come anytime you like. ['like'] ['like']\n",
      "Come anytime. [] []\n",
      "Come as early as possible. [] []\n",
      "Come as soon as possible. [] []\n",
      "Come as soon as you can. ['can'] ['can']\n",
      "Come at any time you like. ['like'] ['like']\n",
      "Come at once. [] []\n",
      "Come at ten o'clock sharp. [] []\n",
      "Come back home. [] []\n",
      "Come back soon. [] []\n",
      "Come back within a month. [] []\n",
      "Come back. [] []\n",
      "Come help me. [] []\n",
      "Come here at exactly six o'clock. [] []\n",
      "Come here at precisely six o'clock. [] []\n",
      "Come here quickly. [] []\n",
      "Come here. [] []\n",
      "Come here. I want to show you something. ['want'] ['want']\n",
      "Come home as soon as you can. ['can'] ['can']\n",
      "Come home at six. [] []\n",
      "Come home before it gets dark. ['gets'] ['get']\n",
      "Come home before six. [] []\n",
      "Come home right away. [] []\n",
      "Come home. [] []\n",
      "Come in, the door's open. [] []\n",
      "Come in. [] []\n",
      "Come inside because it's cold outside. [\"'s\"] ['be']\n",
      "Come inside. [] []\n",
      "Come into my office. [] []\n",
      "Come into the room after me. [] []\n",
      "Come into the room. [] []\n",
      "Come on any day you like. ['like'] ['like']\n",
      "Come on in! [] []\n",
      "Come on in. [] []\n",
      "Come on into my office. [] []\n",
      "Come on! Give me a chance. [] []\n",
      "Come on! We'll be late. ['be'] ['be']\n",
      "Come on, Tom, think about it. [] []\n",
      "Come on, follow me. [] []\n",
      "Come on, spit it out! [] []\n",
      "Come on. [] []\n",
      "Come out with your hands up. [] []\n"
     ]
    }
   ],
   "source": [
    "from spacy.tokens import Doc\n",
    "# lang=load_langs.langs[188]\n",
    "for lang in load_langs.langs[2000:2120]:\n",
    "    doc = Doc(nlp.vocab).from_bytes(lang.store)\n",
    "    print(lang.entries[0], lang.verbs, lang.verbLemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-31T08:09:41.965186Z",
     "start_time": "2019-05-31T08:09:41.761239Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36763\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "englist=[]\n",
    "for lang in load_langs.langs:\n",
    "    englist.append(lang.entries[0])\n",
    "x=np.array(englist) \n",
    "rs=np.unique(x)\n",
    "print(len(rs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-31T06:20:00.008553Z",
     "start_time": "2019-05-31T06:19:59.965014Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"A stitch in time saves nine\" is a proverb.\n",
      "\" \" PUNCT `` punct \" False False\n",
      "A a DET DT det X True False\n",
      "stitch stitch NOUN NN nsubj xxxx True False\n",
      "in in ADP IN prep xx True True\n",
      "time time NOUN NN pobj xxxx True False\n",
      "saves save VERB VBZ csubj xxxx True False\n",
      "nine nine NUM CD dobj xxxx True True\n",
      "\" \" PUNCT `` punct \" False False\n",
      "is be VERB VBZ ROOT xx True True\n",
      "a a DET DT det x True True\n",
      "proverb proverb NOUN NN attr xxxx True False\n",
      ". . PUNCT . punct . False False\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dep</th>\n",
       "      <th>lemma</th>\n",
       "      <th>pos</th>\n",
       "      <th>tag</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>punct</td>\n",
       "      <td>\"</td>\n",
       "      <td>PUNCT</td>\n",
       "      <td>``</td>\n",
       "      <td>\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>det</td>\n",
       "      <td>a</td>\n",
       "      <td>DET</td>\n",
       "      <td>DT</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>nsubj</td>\n",
       "      <td>stitch</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>NN</td>\n",
       "      <td>stitch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>prep</td>\n",
       "      <td>in</td>\n",
       "      <td>ADP</td>\n",
       "      <td>IN</td>\n",
       "      <td>in</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>pobj</td>\n",
       "      <td>time</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>NN</td>\n",
       "      <td>time</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>csubj</td>\n",
       "      <td>save</td>\n",
       "      <td>VERB</td>\n",
       "      <td>VBZ</td>\n",
       "      <td>saves</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>dobj</td>\n",
       "      <td>nine</td>\n",
       "      <td>NUM</td>\n",
       "      <td>CD</td>\n",
       "      <td>nine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>punct</td>\n",
       "      <td>\"</td>\n",
       "      <td>PUNCT</td>\n",
       "      <td>``</td>\n",
       "      <td>\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ROOT</td>\n",
       "      <td>be</td>\n",
       "      <td>VERB</td>\n",
       "      <td>VBZ</td>\n",
       "      <td>is</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>det</td>\n",
       "      <td>a</td>\n",
       "      <td>DET</td>\n",
       "      <td>DT</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>attr</td>\n",
       "      <td>proverb</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>NN</td>\n",
       "      <td>proverb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>punct</td>\n",
       "      <td>.</td>\n",
       "      <td>PUNCT</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      dep    lemma    pos  tag     text\n",
       "0   punct        \"  PUNCT   ``        \"\n",
       "1     det        a    DET   DT        A\n",
       "2   nsubj   stitch   NOUN   NN   stitch\n",
       "3    prep       in    ADP   IN       in\n",
       "4    pobj     time   NOUN   NN     time\n",
       "5   csubj     save   VERB  VBZ    saves\n",
       "6    dobj     nine    NUM   CD     nine\n",
       "7   punct        \"  PUNCT   ``        \"\n",
       "8    ROOT       be   VERB  VBZ       is\n",
       "9     det        a    DET   DT        a\n",
       "10   attr  proverb   NOUN   NN  proverb\n",
       "11  punct        .  PUNCT    .        ."
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(rs[0])\n",
    "doc=nlp(str(rs[0]))\n",
    "doc_df(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-31T07:30:51.185660Z",
     "start_time": "2019-05-31T07:30:51.175694Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36763\n",
      "sampling with choices function  ['I look forward to my birthday.', \"Don't throw in the towel.\", 'There are a lot of trees around the pond.', \"I'm proud to be a part of this project.\", 'He is still very much alive.']\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "print(len(rs))\n",
    "sampling = random.choices(rs, k=5)\n",
    "print(\"sampling with choices function \", sampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-31T08:15:51.822229Z",
     "start_time": "2019-05-31T08:15:51.784148Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing 2D Array\n",
      "[[11 22 33]\n",
      " [44 55 66]\n",
      " [77 88 99]]\n",
      "Choose random row from 2D array\n",
      "[0 0]\n",
      "[[11 22 33]\n",
      " [11 22 33]]\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "array = numpy.array([[11 ,22, 33], [44, 55, 66], [77, 88, 99]]) \n",
    "print(\"Printing 2D Array\")\n",
    "print(array)\n",
    "print(\"Choose random row from 2D array\")\n",
    "randomRow = numpy.random.randint(3, size=2)\n",
    "print(randomRow)\n",
    "print(array[randomRow,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-31T08:11:39.026728Z",
     "start_time": "2019-05-31T08:11:39.020288Z"
    }
   },
   "outputs": [],
   "source": [
    "?numpy.random.randint"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
