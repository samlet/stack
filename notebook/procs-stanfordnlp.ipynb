{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-20T03:28:00.162562Z",
     "start_time": "2019-07-20T03:27:58.604616Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use device: cpu\n",
      "---\n",
      "Loading: tokenize\n",
      "With settings: \n",
      "{'model_path': '/pi/ai/corenlp/en_ewt_models/en_ewt_tokenizer.pt', 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}\n",
      "---\n",
      "Loading: pos\n",
      "With settings: \n",
      "{'model_path': '/pi/ai/corenlp/en_ewt_models/en_ewt_tagger.pt', 'pretrain_path': '/pi/ai/corenlp/en_ewt_models/en_ewt.pretrain.pt', 'batch_size': 3000, 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}\n",
      "Done loading processors!\n",
      "---\n",
      "<Token index=1;words=[<Word index=1;text=Barack;upos=PROPN;xpos=NNP;feats=Number=Sing>]>\n",
      "<Token index=2;words=[<Word index=2;text=Obama;upos=PROPN;xpos=NNP;feats=Number=Sing>]>\n",
      "<Token index=3;words=[<Word index=3;text=was;upos=AUX;xpos=VBD;feats=Mood=Ind|Number=Sing|Person=3|Tense=Past|VerbForm=Fin>]>\n",
      "<Token index=4;words=[<Word index=4;text=born;upos=VERB;xpos=VBN;feats=Tense=Past|VerbForm=Part|Voice=Pass>]>\n",
      "<Token index=5;words=[<Word index=5;text=in;upos=ADP;xpos=IN;feats=_>]>\n",
      "<Token index=6;words=[<Word index=6;text=Hawaii;upos=PROPN;xpos=NNP;feats=Number=Sing>]>\n",
      "<Token index=7;words=[<Word index=7;text=.;upos=PUNCT;xpos=.;feats=_>]>\n"
     ]
    }
   ],
   "source": [
    "import stanfordnlp\n",
    "\n",
    "MODELS_DIR = '/pi/ai/corenlp'\n",
    "nlp = stanfordnlp.Pipeline(processors='tokenize,pos', models_dir=MODELS_DIR, treebank='en_ewt', use_gpu=True, pos_batch_size=3000) # Build the pipeline, specify part-of-speech processor's batch size\n",
    "doc = nlp(\"Barack Obama was born in Hawaii.\") # Run the pipeline on input text\n",
    "doc.sentences[0].print_tokens() # Look at the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-20T03:28:34.682825Z",
     "start_time": "2019-07-20T03:28:32.247707Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use device: cpu\n",
      "---\n",
      "Loading: tokenize\n",
      "With settings: \n",
      "{'model_path': '/pi/ai/corenlp/en_ewt_models/en_ewt_tokenizer.pt', 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}\n",
      "---\n",
      "Loading: pos\n",
      "With settings: \n",
      "{'model_path': '/pi/ai/corenlp/en_ewt_models/en_ewt_tagger.pt', 'pretrain_path': '/pi/ai/corenlp/en_ewt_models/en_ewt.pretrain.pt', 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}\n",
      "---\n",
      "Loading: lemma\n",
      "With settings: \n",
      "{'model_path': '/pi/ai/corenlp/en_ewt_models/en_ewt_lemmatizer.pt', 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}\n",
      "Building an attentional Seq2Seq model...\n",
      "Using a Bi-LSTM encoder\n",
      "Using soft attention for LSTM.\n",
      "Finetune all embeddings.\n",
      "[Running seq2seq lemmatizer with edit classifier]\n",
      "---\n",
      "Loading: depparse\n",
      "With settings: \n",
      "{'model_path': '/pi/ai/corenlp/en_ewt_models/en_ewt_parser.pt', 'pretrain_path': '/pi/ai/corenlp/en_ewt_models/en_ewt.pretrain.pt', 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}\n",
      "Done loading processors!\n",
      "---\n",
      "text: Barack \tlemma: Barack\tupos: PROPN\txpos: NNP\n",
      "text: Obama \tlemma: Obama\tupos: PROPN\txpos: NNP\n",
      "text: was \tlemma: be\tupos: AUX\txpos: VBD\n",
      "text: born \tlemma: bear\tupos: VERB\txpos: VBN\n",
      "text: in \tlemma: in\tupos: ADP\txpos: IN\n",
      "text: Hawaii \tlemma: Hawaii\tupos: PROPN\txpos: NNP\n",
      "text: . \tlemma: .\tupos: PUNCT\txpos: .\n"
     ]
    }
   ],
   "source": [
    "nlp = stanfordnlp.Pipeline(models_dir=MODELS_DIR, treebank='en_ewt')\n",
    "doc = nlp(\"Barack Obama was born in Hawaii.\")\n",
    "print(*[f'text: {word.text+\" \"}\\tlemma: {word.lemma}\\tupos: {word.upos}\\txpos: {word.xpos}' for sent in doc.sentences for word in sent.words], sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-20T03:28:51.521739Z",
     "start_time": "2019-07-20T03:28:51.367467Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text: Barack \tlemma: Barack\tupos: PROPN\txpos: NNP\n",
      "text: Obama \tlemma: Obama\tupos: PROPN\txpos: NNP\n",
      "text: was \tlemma: be\tupos: AUX\txpos: VBD\n",
      "text: born \tlemma: bear\tupos: VERB\txpos: VBN\n",
      "text: in \tlemma: in\tupos: ADP\txpos: IN\n",
      "text: Hawaii \tlemma: Hawaii\tupos: PROPN\txpos: NNP\n",
      "text: . \tlemma: .\tupos: PUNCT\txpos: .\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Barack Obama was born in Hawaii.\")\n",
    "print(*[f'text: {word.text+\" \"}\\tlemma: {word.lemma}\\tupos: {word.upos}\\txpos: {word.xpos}' for sent in doc.sentences for word in sent.words], sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-20T03:28:54.419773Z",
     "start_time": "2019-07-20T03:28:54.409660Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Barack', '4', 'nsubj:pass')\n",
      "('Obama', '1', 'flat')\n",
      "('was', '4', 'aux:pass')\n",
      "('born', '0', 'root')\n",
      "('in', '6', 'case')\n",
      "('Hawaii', '4', 'obl')\n",
      "('.', '4', 'punct')\n"
     ]
    }
   ],
   "source": [
    "doc.sentences[0].print_dependencies()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-20T04:56:28.750507Z",
     "start_time": "2019-07-20T04:56:28.616810Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Barack 1\n",
      "\t Barack PROPN nsubj:pass 4 born\n",
      "2 Obama 1\n",
      "\t Obama PROPN flat 1 Barack\n",
      "3 was 1\n",
      "\t was AUX aux:pass 4 born\n",
      "4 born 1\n",
      "\t born VERB root 0 .\n",
      "5 in 1\n",
      "\t in ADP case 6 Hawaii\n",
      "6 Hawaii 1\n",
      "\t Hawaii PROPN obl 4 born\n",
      "7 . 1\n",
      "\t . PUNCT punct 4 born\n"
     ]
    }
   ],
   "source": [
    "# 分析依赖关系, 自下而上, 可用于抽取指定关系的子节点集合, 比如此例中的'nsubj:pass'和'obl'\n",
    "# word.governor即为当前word的parent\n",
    "sent=doc.sentences[0]\n",
    "for tok in sent.tokens:\n",
    "    print(tok.index, tok.text, len(tok.words))\n",
    "    for word in tok.words:\n",
    "        print('\\t', word.text, word.upos,  word.dependency_relation, \n",
    "              word.governor, sent.words[word.governor-1].text)\n",
    "        # word.parent_token.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-20T05:24:35.702017Z",
     "start_time": "2019-07-20T05:24:35.239860Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "born 4\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rel</th>\n",
       "      <th>index</th>\n",
       "      <th>text</th>\n",
       "      <th>children</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>nsubj:pass</td>\n",
       "      <td>1</td>\n",
       "      <td>Barack</td>\n",
       "      <td>[Obama, Barack]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aux:pass</td>\n",
       "      <td>3</td>\n",
       "      <td>was</td>\n",
       "      <td>[was]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>obl</td>\n",
       "      <td>6</td>\n",
       "      <td>Hawaii</td>\n",
       "      <td>[in, Hawaii]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>punct</td>\n",
       "      <td>7</td>\n",
       "      <td>.</td>\n",
       "      <td>[.]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          rel index    text         children\n",
       "0  nsubj:pass     1  Barack  [Obama, Barack]\n",
       "1    aux:pass     3     was            [was]\n",
       "2         obl     6  Hawaii     [in, Hawaii]\n",
       "3       punct     7       .              [.]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sagas\n",
    "\n",
    "def equals(a,b):\n",
    "    return str(a)==str(b)\n",
    "\n",
    "def get_children(sent, word, rs):\n",
    "    for c in filter(lambda w: equals(w.governor, word.index), sent.words):\n",
    "        rs.append((c.index, c.text))\n",
    "        get_children(sent, c, rs)\n",
    "        \n",
    "def get_children_list(sent, word, include_self=True):\n",
    "    rs=[]\n",
    "    get_children(sent, word, rs)\n",
    "    result= [w[1] for w in rs]\n",
    "    if include_self:\n",
    "        result.append(word.text)\n",
    "    return result\n",
    "\n",
    "def get_verb_domain(sent, filters):\n",
    "    rs=[]\n",
    "    for word in filter(lambda w: w.upos == \"VERB\", sent.words):\n",
    "        # if money.dep_ in (\"attr\", \"dobj\"):\n",
    "        # print(word.index, word.text)\n",
    "        domains=[]\n",
    "        for c in filter(lambda w: equals(w.governor, word.index), sent.words):\n",
    "            # print('\\t', c.index, c.text, get_children_list(sent, c))\n",
    "            domains.append((c.dependency_relation, c.index, c.text, get_children_list(sent, c)))\n",
    "        rs.append({'verb':word.text, 'index':word.index, 'domains':domains})\n",
    "    return rs\n",
    "\n",
    "r=get_verb_domain(sent, ['obl', 'nsubj:pass'])\n",
    "# print(json.dumps(r, indent=2, ensure_ascii=False))\n",
    "print(r[0]['verb'], r[0]['index'])\n",
    "sagas.to_df(r[0]['domains'], ['rel', 'index', 'text', 'children'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-20T03:39:40.000292Z",
     "start_time": "2019-07-20T03:39:39.994244Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Barack', '4', 'nsubj:pass')\n",
      "('Obama', '1', 'flat')\n",
      "('was', '4', 'aux:pass')\n",
      "('born', '0', 'root')\n",
      "('in', '6', 'case')\n",
      "('Hawaii', '4', 'obl')\n",
      "('.', '4', 'punct')\n"
     ]
    }
   ],
   "source": [
    "print(doc.sentences[0].dependencies_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-28T02:58:38.974222Z",
     "start_time": "2019-05-28T02:58:38.837112Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text: I \tlemma: I\tupos: PRON\txpos: PRP\n",
      "text: am \tlemma: be\tupos: AUX\txpos: VBP\n",
      "text: a \tlemma: a\tupos: DET\txpos: DT\n",
      "text: student \tlemma: student\tupos: NOUN\txpos: NN\n",
      "('I', '4', 'nsubj')\n",
      "('am', '4', 'cop')\n",
      "('a', '4', 'det')\n",
      "('student', '0', 'root')\n"
     ]
    }
   ],
   "source": [
    "def analyse(sents):\n",
    "    doc = nlp(sents)\n",
    "    print(*[f'text: {word.text+\" \"}\\tlemma: {word.lemma}\\tupos: {word.upos}\\txpos: {word.xpos}' for sent in doc.sentences for word in sent.words], sep='\\n')\n",
    "    doc.sentences[0].print_dependencies()\n",
    "\n",
    "sents='I am a student'\n",
    "analyse(sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-28T06:12:21.002248Z",
     "start_time": "2019-05-28T06:12:20.396634Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text: The \tlemma: the\tupos: DET\txpos: DT\n",
      "text: car \tlemma: car\tupos: NOUN\txpos: NN\n",
      "text: is \tlemma: be\tupos: AUX\txpos: VBZ\n",
      "text: red \tlemma: red\tupos: ADJ\txpos: JJ\n",
      "text: . \tlemma: .\tupos: PUNCT\txpos: .\n",
      "('The', '2', 'det')\n",
      "('car', '4', 'nsubj')\n",
      "('is', '4', 'cop')\n",
      "('red', '0', 'root')\n",
      "('.', '4', 'punct')\n"
     ]
    }
   ],
   "source": [
    "analyse('The car is red.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-30T00:14:44.343560Z",
     "start_time": "2019-05-30T00:14:41.884409Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use device: cpu\n",
      "---\n",
      "Loading: tokenize\n",
      "With settings: \n",
      "{'model_path': '/pi/ai/corenlp/en_ewt_models/en_ewt_tokenizer.pt', 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}\n",
      "---\n",
      "Loading: lemma\n",
      "With settings: \n",
      "{'model_path': '/pi/ai/corenlp/en_ewt_models/en_ewt_lemmatizer.pt', 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}\n",
      "Building an attentional Seq2Seq model...\n",
      "Using a Bi-LSTM encoder\n",
      "Using soft attention for LSTM.\n",
      "Finetune all embeddings.\n",
      "[Running seq2seq lemmatizer with edit classifier]\n",
      "---\n",
      "Loading: pos\n",
      "With settings: \n",
      "{'model_path': '/pi/ai/corenlp/en_ewt_models/en_ewt_tagger.pt', 'pretrain_path': '/pi/ai/corenlp/en_ewt_models/en_ewt.pretrain.pt', 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}\n",
      "Done loading processors!\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "import stanfordnlp\n",
    "\n",
    "MODELS_DIR = '/pi/ai/corenlp'\n",
    "nlp = stanfordnlp.Pipeline(processors = \"tokenize,mwt,lemma,pos\", models_dir=MODELS_DIR)\n",
    "doc = nlp(\"\"\"The prospects for Britain’s orderly withdrawal from the European Union on March 29 have receded further, even as MPs rallied to stop a no-deal scenario. An amendment to the draft bill on the termination of London’s membership of the bloc obliges Prime Minister Theresa May to renegotiate her withdrawal agreement with Brussels. A Tory backbencher’s proposal calls on the government to come up with alternatives to the Irish backstop, a central tenet of the deal Britain agreed with the rest of the EU.\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-30T00:14:58.881157Z",
     "start_time": "2019-05-30T00:14:58.866157Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Token index=1;words=[<Word index=1;text=The;lemma=the;upos=DET;xpos=DT;feats=Definite=Def|PronType=Art>]>\n",
      "<Token index=2;words=[<Word index=2;text=prospects;lemma=prospect;upos=NOUN;xpos=NNS;feats=Number=Plur>]>\n",
      "<Token index=3;words=[<Word index=3;text=for;lemma=for;upos=ADP;xpos=IN;feats=_>]>\n",
      "<Token index=4;words=[<Word index=4;text=Britain;lemma=Britain;upos=PROPN;xpos=NNP;feats=Number=Sing>]>\n",
      "<Token index=5;words=[<Word index=5;text=’s;lemma='s;upos=PART;xpos=POS;feats=_>]>\n",
      "<Token index=6;words=[<Word index=6;text=orderly;lemma=orderly;upos=ADJ;xpos=JJ;feats=Degree=Pos>]>\n",
      "<Token index=7;words=[<Word index=7;text=withdrawal;lemma=withdrawal;upos=NOUN;xpos=NN;feats=Number=Sing>]>\n",
      "<Token index=8;words=[<Word index=8;text=from;lemma=from;upos=ADP;xpos=IN;feats=_>]>\n",
      "<Token index=9;words=[<Word index=9;text=the;lemma=the;upos=DET;xpos=DT;feats=Definite=Def|PronType=Art>]>\n",
      "<Token index=10;words=[<Word index=10;text=European;lemma=european;upos=ADJ;xpos=JJ;feats=Degree=Pos>]>\n",
      "<Token index=11;words=[<Word index=11;text=Union;lemma=union;upos=PROPN;xpos=NNP;feats=Number=Sing>]>\n",
      "<Token index=12;words=[<Word index=12;text=on;lemma=on;upos=ADP;xpos=IN;feats=_>]>\n",
      "<Token index=13;words=[<Word index=13;text=March;lemma=March;upos=PROPN;xpos=NNP;feats=Number=Sing>]>\n",
      "<Token index=14;words=[<Word index=14;text=29;lemma=29;upos=NUM;xpos=CD;feats=NumType=Card>]>\n",
      "<Token index=15;words=[<Word index=15;text=have;lemma=have;upos=AUX;xpos=VBP;feats=Mood=Ind|Tense=Pres|VerbForm=Fin>]>\n",
      "<Token index=16;words=[<Word index=16;text=receded;lemma=recede;upos=VERB;xpos=VBN;feats=Tense=Past|VerbForm=Part>]>\n",
      "<Token index=17;words=[<Word index=17;text=further;lemma=further;upos=ADV;xpos=RB;feats=_>]>\n",
      "<Token index=18;words=[<Word index=18;text=,;lemma=,;upos=PUNCT;xpos=,;feats=_>]>\n",
      "<Token index=19;words=[<Word index=19;text=even;lemma=even;upos=ADV;xpos=RB;feats=_>]>\n",
      "<Token index=20;words=[<Word index=20;text=as;lemma=as;upos=SCONJ;xpos=IN;feats=_>]>\n",
      "<Token index=21;words=[<Word index=21;text=MPs;lemma=mps;upos=PROPN;xpos=NNPS;feats=Number=Plur>]>\n",
      "<Token index=22;words=[<Word index=22;text=rallied;lemma=rally;upos=VERB;xpos=VBD;feats=Mood=Ind|Tense=Past|VerbForm=Fin>]>\n",
      "<Token index=23;words=[<Word index=23;text=to;lemma=to;upos=PART;xpos=TO;feats=_>]>\n",
      "<Token index=24;words=[<Word index=24;text=stop;lemma=stop;upos=VERB;xpos=VB;feats=VerbForm=Inf>]>\n",
      "<Token index=25;words=[<Word index=25;text=a;lemma=a;upos=DET;xpos=DT;feats=Definite=Ind|PronType=Art>]>\n",
      "<Token index=26;words=[<Word index=26;text=no-deal;lemma=no-deal;upos=ADJ;xpos=JJ;feats=Degree=Pos>]>\n",
      "<Token index=27;words=[<Word index=27;text=scenario;lemma=scenario;upos=NOUN;xpos=NN;feats=Number=Sing>]>\n",
      "<Token index=28;words=[<Word index=28;text=.;lemma=.;upos=PUNCT;xpos=.;feats=_>]>\n"
     ]
    }
   ],
   "source": [
    "doc.sentences[0].print_tokens()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-30T00:15:50.838229Z",
     "start_time": "2019-05-30T00:15:50.303119Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lemma</th>\n",
       "      <th>word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the</td>\n",
       "      <td>The</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>prospect</td>\n",
       "      <td>prospects</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>for</td>\n",
       "      <td>for</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Britain</td>\n",
       "      <td>Britain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>'s</td>\n",
       "      <td>’s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>orderly</td>\n",
       "      <td>orderly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>withdrawal</td>\n",
       "      <td>withdrawal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>from</td>\n",
       "      <td>from</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>the</td>\n",
       "      <td>the</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>european</td>\n",
       "      <td>European</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>union</td>\n",
       "      <td>Union</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>on</td>\n",
       "      <td>on</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>March</td>\n",
       "      <td>March</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>29</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>have</td>\n",
       "      <td>have</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>recede</td>\n",
       "      <td>receded</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>further</td>\n",
       "      <td>further</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>,</td>\n",
       "      <td>,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>even</td>\n",
       "      <td>even</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>as</td>\n",
       "      <td>as</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>mps</td>\n",
       "      <td>MPs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>rally</td>\n",
       "      <td>rallied</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>to</td>\n",
       "      <td>to</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>stop</td>\n",
       "      <td>stop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>no-deal</td>\n",
       "      <td>no-deal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>scenario</td>\n",
       "      <td>scenario</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>a</td>\n",
       "      <td>An</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>amendment</td>\n",
       "      <td>amendment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>proposal</td>\n",
       "      <td>proposal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>call</td>\n",
       "      <td>calls</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>on</td>\n",
       "      <td>on</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>the</td>\n",
       "      <td>the</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>government</td>\n",
       "      <td>government</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>to</td>\n",
       "      <td>to</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>come</td>\n",
       "      <td>come</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>up</td>\n",
       "      <td>up</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>with</td>\n",
       "      <td>with</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>alternative</td>\n",
       "      <td>alternatives</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>to</td>\n",
       "      <td>to</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>the</td>\n",
       "      <td>the</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>irish</td>\n",
       "      <td>Irish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>backstop</td>\n",
       "      <td>backstop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>,</td>\n",
       "      <td>,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>central</td>\n",
       "      <td>central</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>tenet</td>\n",
       "      <td>tenet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>of</td>\n",
       "      <td>of</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>the</td>\n",
       "      <td>the</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>deal</td>\n",
       "      <td>deal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>Britain</td>\n",
       "      <td>Britain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>agree</td>\n",
       "      <td>agreed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>with</td>\n",
       "      <td>with</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>the</td>\n",
       "      <td>the</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>rest</td>\n",
       "      <td>rest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>of</td>\n",
       "      <td>of</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>the</td>\n",
       "      <td>the</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>EU</td>\n",
       "      <td>EU</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>91 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          lemma          word\n",
       "0           the           The\n",
       "1      prospect     prospects\n",
       "2           for           for\n",
       "3       Britain       Britain\n",
       "4            's            ’s\n",
       "5       orderly       orderly\n",
       "6    withdrawal    withdrawal\n",
       "7          from          from\n",
       "8           the           the\n",
       "9      european      European\n",
       "10        union         Union\n",
       "11           on            on\n",
       "12        March         March\n",
       "13           29            29\n",
       "14         have          have\n",
       "15       recede       receded\n",
       "16      further       further\n",
       "17            ,             ,\n",
       "18         even          even\n",
       "19           as            as\n",
       "20          mps           MPs\n",
       "21        rally       rallied\n",
       "22           to            to\n",
       "23         stop          stop\n",
       "24            a             a\n",
       "25      no-deal       no-deal\n",
       "26     scenario      scenario\n",
       "27            .             .\n",
       "28            a            An\n",
       "29    amendment     amendment\n",
       "..          ...           ...\n",
       "61     proposal      proposal\n",
       "62         call         calls\n",
       "63           on            on\n",
       "64          the           the\n",
       "65   government    government\n",
       "66           to            to\n",
       "67         come          come\n",
       "68           up            up\n",
       "69         with          with\n",
       "70  alternative  alternatives\n",
       "71           to            to\n",
       "72          the           the\n",
       "73        irish         Irish\n",
       "74     backstop      backstop\n",
       "75            ,             ,\n",
       "76            a             a\n",
       "77      central       central\n",
       "78        tenet         tenet\n",
       "79           of            of\n",
       "80          the           the\n",
       "81         deal          deal\n",
       "82      Britain       Britain\n",
       "83        agree        agreed\n",
       "84         with          with\n",
       "85          the           the\n",
       "86         rest          rest\n",
       "87           of            of\n",
       "88          the           the\n",
       "89           EU            EU\n",
       "90            .             .\n",
       "\n",
       "[91 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#extract lemma\n",
    "def extract_lemma(doc):\n",
    "    parsed_text = {'word':[], 'lemma':[]}\n",
    "    for sent in doc.sentences:\n",
    "        for wrd in sent.words:\n",
    "            #extract text and lemma\n",
    "            parsed_text['word'].append(wrd.text)\n",
    "            parsed_text['lemma'].append(wrd.lemma)\n",
    "    #return a dataframe\n",
    "    return pd.DataFrame(parsed_text)\n",
    "\n",
    "#call the function on doc\n",
    "extract_lemma(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-30T00:17:37.499047Z",
     "start_time": "2019-05-30T00:17:37.399213Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>exp</th>\n",
       "      <th>pos</th>\n",
       "      <th>word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>determiner</td>\n",
       "      <td>DT</td>\n",
       "      <td>The</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>noun plural 'desks'</td>\n",
       "      <td>NNS</td>\n",
       "      <td>prospects</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>preposition/subordinating conjunction</td>\n",
       "      <td>IN</td>\n",
       "      <td>for</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>proper noun, singular 'Harrison'</td>\n",
       "      <td>NNP</td>\n",
       "      <td>Britain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>possessive ending parent's</td>\n",
       "      <td>POS</td>\n",
       "      <td>’s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>adjective 'big'</td>\n",
       "      <td>JJ</td>\n",
       "      <td>orderly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>noun, singular 'desk'</td>\n",
       "      <td>NN</td>\n",
       "      <td>withdrawal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>preposition/subordinating conjunction</td>\n",
       "      <td>IN</td>\n",
       "      <td>from</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>determiner</td>\n",
       "      <td>DT</td>\n",
       "      <td>the</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>adjective 'big'</td>\n",
       "      <td>JJ</td>\n",
       "      <td>European</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>proper noun, singular 'Harrison'</td>\n",
       "      <td>NNP</td>\n",
       "      <td>Union</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>preposition/subordinating conjunction</td>\n",
       "      <td>IN</td>\n",
       "      <td>on</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>proper noun, singular 'Harrison'</td>\n",
       "      <td>NNP</td>\n",
       "      <td>March</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>cardinal digit</td>\n",
       "      <td>CD</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>verb, sing. present, non-3d take</td>\n",
       "      <td>VBP</td>\n",
       "      <td>have</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>verb, past participle taken</td>\n",
       "      <td>VBN</td>\n",
       "      <td>receded</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>adverb very, silently,</td>\n",
       "      <td>RB</td>\n",
       "      <td>further</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>NA</td>\n",
       "      <td>,</td>\n",
       "      <td>,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>adverb very, silently,</td>\n",
       "      <td>RB</td>\n",
       "      <td>even</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>preposition/subordinating conjunction</td>\n",
       "      <td>IN</td>\n",
       "      <td>as</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>proper noun, plural 'Americans'</td>\n",
       "      <td>NNPS</td>\n",
       "      <td>MPs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>verb, past tense took</td>\n",
       "      <td>VBD</td>\n",
       "      <td>rallied</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>to go 'to' the store.</td>\n",
       "      <td>TO</td>\n",
       "      <td>to</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>verb, base form take</td>\n",
       "      <td>VB</td>\n",
       "      <td>stop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>determiner</td>\n",
       "      <td>DT</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>adjective 'big'</td>\n",
       "      <td>JJ</td>\n",
       "      <td>no-deal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>noun, singular 'desk'</td>\n",
       "      <td>NN</td>\n",
       "      <td>scenario</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>NA</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>determiner</td>\n",
       "      <td>DT</td>\n",
       "      <td>An</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>noun, singular 'desk'</td>\n",
       "      <td>NN</td>\n",
       "      <td>amendment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>noun, singular 'desk'</td>\n",
       "      <td>NN</td>\n",
       "      <td>proposal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>verb, 3rd person sing. present takes</td>\n",
       "      <td>VBZ</td>\n",
       "      <td>calls</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>preposition/subordinating conjunction</td>\n",
       "      <td>IN</td>\n",
       "      <td>on</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>determiner</td>\n",
       "      <td>DT</td>\n",
       "      <td>the</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>noun, singular 'desk'</td>\n",
       "      <td>NN</td>\n",
       "      <td>government</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>to go 'to' the store.</td>\n",
       "      <td>TO</td>\n",
       "      <td>to</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>verb, base form take</td>\n",
       "      <td>VB</td>\n",
       "      <td>come</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>adverb very, silently,</td>\n",
       "      <td>RB</td>\n",
       "      <td>up</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>preposition/subordinating conjunction</td>\n",
       "      <td>IN</td>\n",
       "      <td>with</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>noun plural 'desks'</td>\n",
       "      <td>NNS</td>\n",
       "      <td>alternatives</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>preposition/subordinating conjunction</td>\n",
       "      <td>IN</td>\n",
       "      <td>to</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>determiner</td>\n",
       "      <td>DT</td>\n",
       "      <td>the</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>adjective 'big'</td>\n",
       "      <td>JJ</td>\n",
       "      <td>Irish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>noun, singular 'desk'</td>\n",
       "      <td>NN</td>\n",
       "      <td>backstop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>NA</td>\n",
       "      <td>,</td>\n",
       "      <td>,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>determiner</td>\n",
       "      <td>DT</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>adjective 'big'</td>\n",
       "      <td>JJ</td>\n",
       "      <td>central</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>noun, singular 'desk'</td>\n",
       "      <td>NN</td>\n",
       "      <td>tenet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>preposition/subordinating conjunction</td>\n",
       "      <td>IN</td>\n",
       "      <td>of</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>determiner</td>\n",
       "      <td>DT</td>\n",
       "      <td>the</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>noun, singular 'desk'</td>\n",
       "      <td>NN</td>\n",
       "      <td>deal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>proper noun, singular 'Harrison'</td>\n",
       "      <td>NNP</td>\n",
       "      <td>Britain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>verb, past tense took</td>\n",
       "      <td>VBD</td>\n",
       "      <td>agreed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>preposition/subordinating conjunction</td>\n",
       "      <td>IN</td>\n",
       "      <td>with</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>determiner</td>\n",
       "      <td>DT</td>\n",
       "      <td>the</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>noun, singular 'desk'</td>\n",
       "      <td>NN</td>\n",
       "      <td>rest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>preposition/subordinating conjunction</td>\n",
       "      <td>IN</td>\n",
       "      <td>of</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>determiner</td>\n",
       "      <td>DT</td>\n",
       "      <td>the</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>proper noun, singular 'Harrison'</td>\n",
       "      <td>NNP</td>\n",
       "      <td>EU</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>NA</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>91 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      exp   pos          word\n",
       "0                              determiner    DT           The\n",
       "1                     noun plural 'desks'   NNS     prospects\n",
       "2   preposition/subordinating conjunction    IN           for\n",
       "3        proper noun, singular 'Harrison'   NNP       Britain\n",
       "4              possessive ending parent's   POS            ’s\n",
       "5                         adjective 'big'    JJ       orderly\n",
       "6                   noun, singular 'desk'    NN    withdrawal\n",
       "7   preposition/subordinating conjunction    IN          from\n",
       "8                              determiner    DT           the\n",
       "9                         adjective 'big'    JJ      European\n",
       "10       proper noun, singular 'Harrison'   NNP         Union\n",
       "11  preposition/subordinating conjunction    IN            on\n",
       "12       proper noun, singular 'Harrison'   NNP         March\n",
       "13                         cardinal digit    CD            29\n",
       "14       verb, sing. present, non-3d take   VBP          have\n",
       "15            verb, past participle taken   VBN       receded\n",
       "16                 adverb very, silently,    RB       further\n",
       "17                                     NA     ,             ,\n",
       "18                 adverb very, silently,    RB          even\n",
       "19  preposition/subordinating conjunction    IN            as\n",
       "20        proper noun, plural 'Americans'  NNPS           MPs\n",
       "21                  verb, past tense took   VBD       rallied\n",
       "22                  to go 'to' the store.    TO            to\n",
       "23                   verb, base form take    VB          stop\n",
       "24                             determiner    DT             a\n",
       "25                        adjective 'big'    JJ       no-deal\n",
       "26                  noun, singular 'desk'    NN      scenario\n",
       "27                                     NA     .             .\n",
       "28                             determiner    DT            An\n",
       "29                  noun, singular 'desk'    NN     amendment\n",
       "..                                    ...   ...           ...\n",
       "61                  noun, singular 'desk'    NN      proposal\n",
       "62   verb, 3rd person sing. present takes   VBZ         calls\n",
       "63  preposition/subordinating conjunction    IN            on\n",
       "64                             determiner    DT           the\n",
       "65                  noun, singular 'desk'    NN    government\n",
       "66                  to go 'to' the store.    TO            to\n",
       "67                   verb, base form take    VB          come\n",
       "68                 adverb very, silently,    RB            up\n",
       "69  preposition/subordinating conjunction    IN          with\n",
       "70                    noun plural 'desks'   NNS  alternatives\n",
       "71  preposition/subordinating conjunction    IN            to\n",
       "72                             determiner    DT           the\n",
       "73                        adjective 'big'    JJ         Irish\n",
       "74                  noun, singular 'desk'    NN      backstop\n",
       "75                                     NA     ,             ,\n",
       "76                             determiner    DT             a\n",
       "77                        adjective 'big'    JJ       central\n",
       "78                  noun, singular 'desk'    NN         tenet\n",
       "79  preposition/subordinating conjunction    IN            of\n",
       "80                             determiner    DT           the\n",
       "81                  noun, singular 'desk'    NN          deal\n",
       "82       proper noun, singular 'Harrison'   NNP       Britain\n",
       "83                  verb, past tense took   VBD        agreed\n",
       "84  preposition/subordinating conjunction    IN          with\n",
       "85                             determiner    DT           the\n",
       "86                  noun, singular 'desk'    NN          rest\n",
       "87  preposition/subordinating conjunction    IN            of\n",
       "88                             determiner    DT           the\n",
       "89       proper noun, singular 'Harrison'   NNP            EU\n",
       "90                                     NA     .             .\n",
       "\n",
       "[91 rows x 3 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#dictionary that contains pos tags and their explanations\n",
    "pos_dict = {\n",
    "'CC': 'coordinating conjunction','CD': 'cardinal digit','DT': 'determiner',\n",
    "'EX': 'existential there (like: \\\"there is\\\" ... think of it like \\\"there exists\\\")',\n",
    "'FW': 'foreign word','IN':  'preposition/subordinating conjunction','JJ': 'adjective \\'big\\'',\n",
    "'JJR': 'adjective, comparative \\'bigger\\'','JJS': 'adjective, superlative \\'biggest\\'',\n",
    "'LS': 'list marker 1)','MD': 'modal could, will','NN': 'noun, singular \\'desk\\'',\n",
    "'NNS': 'noun plural \\'desks\\'','NNP': 'proper noun, singular \\'Harrison\\'',\n",
    "'NNPS': 'proper noun, plural \\'Americans\\'','PDT': 'predeterminer \\'all the kids\\'',\n",
    "'POS': 'possessive ending parent\\'s','PRP': 'personal pronoun I, he, she',\n",
    "'PRP$': 'possessive pronoun my, his, hers','RB': 'adverb very, silently,',\n",
    "'RBR': 'adverb, comparative better','RBS': 'adverb, superlative best',\n",
    "'RP': 'particle give up','TO': 'to go \\'to\\' the store.','UH': 'interjection errrrrrrrm',\n",
    "'VB': 'verb, base form take','VBD': 'verb, past tense took',\n",
    "'VBG': 'verb, gerund/present participle taking','VBN': 'verb, past participle taken',\n",
    "'VBP': 'verb, sing. present, non-3d take','VBZ': 'verb, 3rd person sing. present takes',\n",
    "'WDT': 'wh-determiner which','WP': 'wh-pronoun who, what','WP$': 'possessive wh-pronoun whose',\n",
    "'WRB': 'wh-abverb where, when','QF' : 'quantifier, bahut, thoda, kam (Hindi)','VM' : 'main verb',\n",
    "'PSP' : 'postposition, common in indian langs','DEM' : 'demonstrative, common in indian langs'\n",
    "}\n",
    "\n",
    "#extract parts of speech\n",
    "def extract_pos(doc):\n",
    "    parsed_text = {'word':[], 'pos':[], 'exp':[]}\n",
    "    for sent in doc.sentences:\n",
    "        for wrd in sent.words:\n",
    "            if wrd.pos in pos_dict.keys():\n",
    "                pos_exp = pos_dict[wrd.pos]\n",
    "            else:\n",
    "                pos_exp = 'NA'\n",
    "            parsed_text['word'].append(wrd.text)\n",
    "            parsed_text['pos'].append(wrd.pos)\n",
    "            parsed_text['exp'].append(pos_exp)\n",
    "    #return a dataframe of pos and text\n",
    "    return pd.DataFrame(parsed_text)\n",
    "\n",
    "#extract pos\n",
    "extract_pos(doc)\n",
    "\n",
    "# The output would be a data frame with three columns \n",
    "# – word, pos and exp (explanation). \n",
    "# The explanation column gives us the most information \n",
    "# about the text (and is hence quite useful)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-30T00:23:52.569686Z",
     "start_time": "2019-05-30T00:23:48.896460Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use device: cpu\n",
      "---\n",
      "Loading: tokenize\n",
      "With settings: \n",
      "{'model_path': '/pi/ai/corenlp/en_ewt_models/en_ewt_tokenizer.pt', 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}\n",
      "---\n",
      "Loading: lemma\n",
      "With settings: \n",
      "{'model_path': '/pi/ai/corenlp/en_ewt_models/en_ewt_lemmatizer.pt', 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}\n",
      "Building an attentional Seq2Seq model...\n",
      "Using a Bi-LSTM encoder\n",
      "Using soft attention for LSTM.\n",
      "Finetune all embeddings.\n",
      "[Running seq2seq lemmatizer with edit classifier]\n",
      "---\n",
      "Loading: pos\n",
      "With settings: \n",
      "{'model_path': '/pi/ai/corenlp/en_ewt_models/en_ewt_tagger.pt', 'pretrain_path': '/pi/ai/corenlp/en_ewt_models/en_ewt.pretrain.pt', 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}\n",
      "---\n",
      "Loading: depparse\n",
      "With settings: \n",
      "{'model_path': '/pi/ai/corenlp/en_ewt_models/en_ewt_parser.pt', 'pretrain_path': '/pi/ai/corenlp/en_ewt_models/en_ewt.pretrain.pt', 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}\n",
      "Done loading processors!\n",
      "---\n",
      "3\n",
      "('The', '2', 'det')\n",
      "('prospects', '16', 'nsubj')\n",
      "('for', '7', 'case')\n",
      "('Britain', '7', 'nmod:poss')\n",
      "('’s', '4', 'case')\n",
      "('orderly', '7', 'amod')\n",
      "('withdrawal', '2', 'nmod')\n",
      "('from', '11', 'case')\n",
      "('the', '11', 'det')\n",
      "('European', '11', 'amod')\n",
      "('Union', '7', 'nmod')\n",
      "('on', '13', 'case')\n",
      "('March', '11', 'nmod')\n",
      "('29', '13', 'nummod')\n",
      "('have', '16', 'aux')\n",
      "('receded', '0', 'root')\n",
      "('further', '16', 'advmod')\n",
      "(',', '16', 'punct')\n",
      "('even', '22', 'advmod')\n",
      "('as', '22', 'mark')\n",
      "('MPs', '22', 'nsubj')\n",
      "('rallied', '16', 'advcl')\n",
      "('to', '24', 'mark')\n",
      "('stop', '22', 'xcomp')\n",
      "('a', '27', 'det')\n",
      "('no-deal', '27', 'amod')\n",
      "('scenario', '24', 'obj')\n",
      "('.', '16', 'punct')\n"
     ]
    }
   ],
   "source": [
    "nlp = stanfordnlp.Pipeline(processors = \"tokenize,mwt,lemma,pos,depparse\", \n",
    "                           models_dir=MODELS_DIR)\n",
    "\n",
    "doc = nlp(\"\"\"The prospects for Britain’s orderly withdrawal from the European Union on March 29 have receded further, even as MPs rallied to stop a no-deal scenario. An amendment to the draft bill on the termination of London’s membership of the bloc obliges Prime Minister Theresa May to renegotiate her withdrawal agreement with Brussels. A Tory backbencher’s proposal calls on the government to come up with alternatives to the Irish backstop, a central tenet of the deal Britain agreed with the rest of the EU.\"\"\")\n",
    "# doc=nlp('I am a student')\n",
    "print(len(doc.sentences))\n",
    "doc.sentences[0].print_dependencies()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
