{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-09T18:58:32.903188Z",
     "start_time": "2019-04-09T18:58:32.888729Z"
    }
   },
   "outputs": [],
   "source": [
    "from whoosh.index import create_in\n",
    "from whoosh.fields import *\n",
    "import os\n",
    "schema = Schema(title=TEXT(stored=True), path=ID(stored=True), content=TEXT)\n",
    "\n",
    "if not os.path.exists(\"out/index\"):\n",
    "    os.mkdir(\"out/index\")\n",
    "ix = create_in(\"out/indexdir\", schema)\n",
    "writer = ix.writer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-15T12:56:44.218267Z",
     "start_time": "2019-03-15T12:56:44.172031Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Hit {'path': '/a', 'title': 'First document'}>\n"
     ]
    }
   ],
   "source": [
    "from whoosh.qparser import QueryParser\n",
    "\n",
    "writer.add_document(title=u\"First document\", path=u\"/a\",\n",
    "                    content=u\"This is the first document we've added!\")\n",
    "writer.add_document(title=u\"Second document\", path=u\"/b\",\n",
    "                    content=u\"The second one is even more interesting!\")\n",
    "writer.commit()\n",
    "\n",
    "with ix.searcher() as searcher:\n",
    "    query = QueryParser(\"content\", ix.schema).parse(\"first\")\n",
    "    results = searcher.search(query)\n",
    "    print(results[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-15T12:57:05.229407Z",
     "start_time": "2019-03-15T12:57:05.199734Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Hit {'path': '/a', 'title': 'First document'}>\n"
     ]
    }
   ],
   "source": [
    "with ix.searcher() as searcher:\n",
    "    query = QueryParser(\"content\", ix.schema).parse(\"first\")\n",
    "    results = searcher.search(query)\n",
    "    print(results[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-09T19:30:34.228019Z",
     "start_time": "2019-04-09T19:30:34.141275Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result of  水果\n",
      "document for test, <b class=\"match term0\">水果</b>和米饭\n",
      "==========\n",
      "result of  你\n",
      "==========\n",
      "result of  first\n",
      "==========\n",
      "result of  test\n",
      "document for <b class=\"match term0\">test</b>, 水果和米饭\n",
      "==========\n",
      "result of  中文\n",
      "==========\n",
      "result of  交换机\n",
      "==========\n",
      "result of  交换\n",
      "==========\n"
     ]
    }
   ],
   "source": [
    "from __future__ import unicode_literals\n",
    "from jieba.analyse import ChineseAnalyzer\n",
    "\n",
    "analyzer = ChineseAnalyzer()\n",
    "\n",
    "schema = Schema(title=TEXT(stored=True), path=ID(stored=True), \n",
    "                content=TEXT(stored=True, analyzer=analyzer))\n",
    "\n",
    "## rewrite mode\n",
    "if not os.path.exists(\"out/test\"):\n",
    "    os.mkdir(\"out/test\")\n",
    "idx = create_in(\"out/test\", schema)\n",
    "\n",
    "writer = idx.writer()\n",
    "writer.add_document(\n",
    "    title=\"first test-document\",\n",
    "    path=\"/c\",\n",
    "    content=\"This is the document for test, 水果和米饭.\"\n",
    ")\n",
    "writer.commit()\n",
    "searcher = idx.searcher()\n",
    "parser = QueryParser(\"content\", schema=idx.schema)\n",
    "\n",
    "for keyword in (\"水果\",\"你\",\"first\", 'test',\"中文\",\"交换机\",\"交换\"):\n",
    "    print(\"result of \",keyword)\n",
    "    q = parser.parse(keyword)\n",
    "    results = searcher.search(q)\n",
    "    for hit in results:\n",
    "        print(hit.highlights(\"content\"))\n",
    "    print(\"=\"*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-09T19:00:16.468694Z",
     "start_time": "2019-04-09T19:00:16.379576Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result of  水果\n",
      "document for test, <b class=\"match term0\">水果</b>和米饭\n",
      "document for test, <b class=\"match term0\">水果</b>和大蒜\n",
      "==========\n",
      "result of  你\n",
      "==========\n",
      "result of  first\n",
      "==========\n",
      "result of  test\n",
      "document for <b class=\"match term0\">test</b>, 水果和米饭\n",
      "document for <b class=\"match term0\">test</b>, 水果和大蒜\n",
      "==========\n",
      "result of  中文\n",
      "==========\n",
      "result of  交换机\n",
      "==========\n",
      "result of  交换\n",
      "==========\n"
     ]
    }
   ],
   "source": [
    "from __future__ import unicode_literals\n",
    "\n",
    "from whoosh.index import open_dir\n",
    "from whoosh.index import create_in\n",
    "from whoosh.fields import *\n",
    "\n",
    "from jieba.analyse import ChineseAnalyzer\n",
    "from whoosh.qparser import QueryParser\n",
    "\n",
    "analyzer = ChineseAnalyzer()\n",
    "schema = Schema(title=TEXT(stored=True), path=ID(stored=True), content=TEXT(stored=True, analyzer=analyzer))\n",
    "\n",
    "## append mode\n",
    "idx = open_dir(\"out/test\")\n",
    "\n",
    "writer = idx.writer()\n",
    "writer.add_document(\n",
    "    title=\"test-document-2\",\n",
    "    path=\"/b\",\n",
    "    content=\"This is the document for test, 水果和大蒜.\"\n",
    ")\n",
    "writer.commit()\n",
    "searcher = idx.searcher()\n",
    "parser = QueryParser(\"content\", schema=idx.schema)\n",
    "\n",
    "for keyword in (\"水果\",\"你\",\"first\", 'test',\"中文\",\"交换机\",\"交换\"):\n",
    "    print(\"result of \",keyword)\n",
    "    q = parser.parse(keyword)\n",
    "    results = searcher.search(q)\n",
    "    for hit in results:\n",
    "        print(hit.highlights(\"content\"))\n",
    "    print(\"=\"*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-09T19:42:27.871510Z",
     "start_time": "2019-04-09T19:42:27.743421Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- SagasUiLabels.xml\n",
      "SagasApplication ⊕ SagasUiLabels.xml\n",
      "\t value Sagas Application en\n",
      "\t value Sagas应用程序 zh\n",
      "\t value Sagas應用程式 zh-TW\n",
      "SagasCompanyName ⊕ SagasUiLabels.xml\n",
      "\t value OFBiz: Sagas en\n",
      "\t value OFBiz: Sagas zh-TW\n",
      "SagasCompanySubtitle ⊕ SagasUiLabels.xml\n",
      "\t value Part of the Apache OFBiz Family of Open Source Software en\n",
      "\t value Un modulo della famiglia di software open source Apache OFBiz it\n",
      "\t value 开源软件OFBiz的组成部分 zh\n",
      "\t value 開源軟體OFBiz的組成部分 zh-TW\n",
      "SagasViewPermissionError ⊕ SagasUiLabels.xml\n",
      "\t value You are not allowed to view this page. en\n",
      "\t value 不允许你浏览这个页面。 zh\n",
      "\t value 不允許您檢視這個頁面. zh-TW\n",
      "result of  中文\n",
      "==========\n",
      "result of  组成部分\n",
      "开源软件OFBiz的<b class=\"match term0\">组成部分</b>\n",
      "==========\n",
      "result of  交换\n",
      "==========\n"
     ]
    }
   ],
   "source": [
    "from __future__ import unicode_literals\n",
    "from jieba.analyse import ChineseAnalyzer\n",
    "from sagas.ofbiz.resources import ResourceDigester\n",
    "\n",
    "rd=ResourceDigester()\n",
    "resource=rd.process_resource(xml_file='data/i18n/SagasUiLabels.xml')\n",
    "\n",
    "analyzer = ChineseAnalyzer()\n",
    "schema = Schema(en=TEXT(stored=True), \n",
    "                fr=TEXT(stored=True),\n",
    "                key=ID(stored=True), \n",
    "                zh=TEXT(stored=True, analyzer=analyzer))\n",
    "\n",
    "## rewrite mode\n",
    "out_dir='out/labels'\n",
    "if not os.path.exists(out_dir):\n",
    "    os.mkdir(out_dir)\n",
    "idx = create_in(out_dir, schema)\n",
    "\n",
    "writer = idx.writer()\n",
    "\n",
    "for key, prop in resource.properties.items():    \n",
    "    writer.add_document(\n",
    "        key=key,\n",
    "        en=prop.values['en'],\n",
    "        zh=prop.values['zh'],\n",
    "        fr=prop.values['fr']\n",
    "    )\n",
    "\n",
    "writer.commit()\n",
    "\n",
    "searcher = idx.searcher()\n",
    "parser = QueryParser(\"zh\", schema=idx.schema)\n",
    "\n",
    "for keyword in (\"中文\",\"组成部分\",\"交换\"):\n",
    "    print(\"result of \",keyword)\n",
    "    q = parser.parse(keyword)\n",
    "    results = searcher.search(q)\n",
    "    for hit in results:\n",
    "        print(hit.highlights(\"zh\"))\n",
    "    print(\"=\"*10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
