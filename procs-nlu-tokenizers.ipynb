{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-26T08:38:29.846479Z",
     "start_time": "2018-12-26T08:38:29.837402Z"
    }
   },
   "outputs": [],
   "source": [
    "from rasa_nlu.tokenizers.whitespace_tokenizer import WhitespaceTokenizer\n",
    "tk = WhitespaceTokenizer()\n",
    "assert [t.text for t in tk.tokenize(\"Forecast for lunch\")] == \\\n",
    "    ['Forecast', 'for', 'lunch']\n",
    "assert [t.text for t in tk.tokenize(\"привет! 10.000, ńöñàśçií. how're you?\")] == \\\n",
    "       ['привет', '10.000', 'ńöñàśçií', 'how\\'re', 'you']\n",
    "\n",
    "assert [t.offset for t in tk.tokenize(\"привет! 10.000, ńöñàśçií. how're you?\")] == \\\n",
    "       [0, 8, 16, 26, 33]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-26T09:24:53.317498Z",
     "start_time": "2018-12-26T09:24:35.751881Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xiaofeiwu/miniconda3/envs/bigdata/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "INFO:rasa_nlu.utils.spacy_utils:Trying to load spacy model with name 'en'\n",
      "INFO:rasa_nlu.components:Added 'nlp_spacy' to component cache. Key 'nlp_spacy-en'.\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import os\n",
    "\n",
    "import pytest\n",
    "from rasa_nlu import data_router, config\n",
    "from rasa_nlu.components import ComponentBuilder\n",
    "from rasa_nlu.model import Trainer\n",
    "from rasa_nlu.utils import zip_folder\n",
    "from rasa_nlu import training_data\n",
    "\n",
    "logging.basicConfig(level=\"DEBUG\")\n",
    "\n",
    "CONFIG_DEFAULTS_PATH = \"sample_configs/config_defaults.yml\"\n",
    "\n",
    "DEFAULT_DATA_PATH = \"data/examples/rasa/demo-rasa.json\"\n",
    "\n",
    "TEST_MODEL_PATH = \"test_models/test_model_spacy_sklearn\"\n",
    "\n",
    "\n",
    "@pytest.fixture(scope=\"session\")\n",
    "def component_builder():\n",
    "    return ComponentBuilder()\n",
    "\n",
    "\n",
    "@pytest.fixture(scope=\"session\")\n",
    "def spacy_nlp(component_builder, default_config):\n",
    "    return component_builder.create_component(\"nlp_spacy\", default_config).nlp\n",
    "\n",
    "\n",
    "@pytest.fixture(scope=\"session\")\n",
    "def ner_crf_pos_feature_config():\n",
    "    return {\n",
    "        \"features\": [\n",
    "            [\"low\", \"title\", \"upper\", \"pos\", \"pos2\"],\n",
    "            [\"bias\", \"low\", \"suffix3\", \"suffix2\", \"upper\",\n",
    "             \"title\", \"digit\", \"pos\", \"pos2\", \"pattern\"],\n",
    "            [\"low\", \"title\", \"upper\", \"pos\", \"pos2\"]]\n",
    "    }\n",
    "\n",
    "\n",
    "@pytest.fixture(scope=\"session\")\n",
    "def mitie_feature_extractor(component_builder, default_config):\n",
    "    return component_builder.create_component(\"nlp_mitie\",\n",
    "                                              default_config).extractor\n",
    "\n",
    "\n",
    "@pytest.fixture(scope=\"session\")\n",
    "def default_config():\n",
    "    return config.load(CONFIG_DEFAULTS_PATH)\n",
    "\n",
    "spacy_nlp=spacy_nlp(component_builder(), default_config())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-26T09:24:58.861303Z",
     "start_time": "2018-12-26T09:24:58.765936Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from rasa_nlu.tokenizers.spacy_tokenizer import SpacyTokenizer\n",
    "tk = SpacyTokenizer()\n",
    "text = \"Forecast for lunch\"\n",
    "assert [t.text for t in tk.tokenize(spacy_nlp(text))] == \\\n",
    "       ['Forecast', 'for', 'lunch']\n",
    "assert [t.offset for t in tk.tokenize(spacy_nlp(text))] == \\\n",
    "       [0, 9, 13]\n",
    "\n",
    "text = \"hey ńöñàśçií how're you?\"\n",
    "assert [t.text for t in tk.tokenize(spacy_nlp(text))] == \\\n",
    "       ['hey', 'ńöñàśçií', 'how', '\\'re', 'you', '?']\n",
    "assert [t.offset for t in tk.tokenize(spacy_nlp(text))] == \\\n",
    "       [0, 4, 13, 16, 20, 23]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-26T09:25:44.784991Z",
     "start_time": "2018-12-26T09:25:43.726617Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "DEBUG:jieba:Building prefix dict from the default dictionary ...\n",
      "Dumping model to file cache /var/folders/fv/7k1qk5v11dn33sdcngv2wbnm0000gn/T/jieba.cache\n",
      "DEBUG:jieba:Dumping model to file cache /var/folders/fv/7k1qk5v11dn33sdcngv2wbnm0000gn/T/jieba.cache\n",
      "Loading model cost 0.999 seconds.\n",
      "DEBUG:jieba:Loading model cost 0.999 seconds.\n",
      "Prefix dict has been built succesfully.\n",
      "DEBUG:jieba:Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "def test_jieba():\n",
    "    from rasa_nlu.tokenizers.jieba_tokenizer import JiebaTokenizer\n",
    "    tk = JiebaTokenizer()\n",
    "\n",
    "    assert [t.text for t in tk.tokenize(\"我想去吃兰州拉面\")] == \\\n",
    "           ['我', '想', '去', '吃', '兰州', '拉面']\n",
    "\n",
    "    assert [t.offset for t in tk.tokenize(\"我想去吃兰州拉面\")] == \\\n",
    "           [0, 1, 2, 3, 4, 6]\n",
    "\n",
    "    assert [t.text for t in tk.tokenize(\"Micheal你好吗？\")] == \\\n",
    "           ['Micheal', '你好', '吗', '？']\n",
    "\n",
    "    assert [t.offset for t in tk.tokenize(\"Micheal你好吗？\")] == \\\n",
    "           [0, 7, 9, 10]\n",
    "\n",
    "test_jieba()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-26T09:26:56.126010Z",
     "start_time": "2018-12-26T09:26:56.088507Z"
    }
   },
   "outputs": [],
   "source": [
    "from rasa_nlu.config import RasaNLUModelConfig\n",
    "from rasa_nlu.extractors.spacy_entity_extractor import SpacyEntityExtractor\n",
    "from rasa_nlu.training_data import TrainingData, Message\n",
    "\n",
    "def test_spacy_ner_extractor(spacy_nlp):\n",
    "    ext = SpacyEntityExtractor()\n",
    "    example = Message(\"anywhere in the West\", {\n",
    "        \"intent\": \"restaurant_search\",\n",
    "        \"entities\": [],\n",
    "        \"spacy_doc\": spacy_nlp(\"anywhere in the west\")})\n",
    "\n",
    "    ext.process(example, spacy_nlp=spacy_nlp)\n",
    "\n",
    "    assert len(example.get(\"entities\", [])) == 1\n",
    "    assert example.get(\"entities\")[0] == {\n",
    "        'start': 16,\n",
    "        'extractor': 'ner_spacy',\n",
    "        'end': 20,\n",
    "        'value': 'West',\n",
    "        'entity': 'LOC',\n",
    "        'confidence': None}\n",
    "\n",
    "test_spacy_ner_extractor(spacy_nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-26T09:29:20.337590Z",
     "start_time": "2018-12-26T09:29:20.133556Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:rasa_nlu.training_data.training_data:Training data stats: \n",
      "\t- intent examples: 2 (1 distinct intents)\n",
      "\t- Found intents: 'restaurant_search'\n",
      "\t- entity examples: 2 (2 distinct entities)\n",
      "\t- found entities: 'cuisine', 'location'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def test_crf_extractor(spacy_nlp, ner_crf_pos_feature_config):\n",
    "    from rasa_nlu.extractors.crf_entity_extractor import CRFEntityExtractor\n",
    "    ext = CRFEntityExtractor(component_config=ner_crf_pos_feature_config)\n",
    "    examples = [\n",
    "        Message(\"anywhere in the west\", {\n",
    "            \"intent\": \"restaurant_search\",\n",
    "            \"entities\": [{\"start\": 16, \"end\": 20,\n",
    "                          \"value\": \"west\", \"entity\": \"location\"}],\n",
    "            \"spacy_doc\": spacy_nlp(\"anywhere in the west\")\n",
    "        }),\n",
    "        Message(\"central indian restaurant\", {\n",
    "            \"intent\": \"restaurant_search\",\n",
    "            \"entities\": [\n",
    "                {\"start\": 0, \"end\": 7, \"value\": \"central\",\n",
    "                 \"entity\": \"location\", \"extractor\": \"random_extractor\"},\n",
    "                {\"start\": 8, \"end\": 14, \"value\": \"indian\",\n",
    "                 \"entity\": \"cuisine\", \"extractor\": \"ner_crf\"}\n",
    "            ],\n",
    "            \"spacy_doc\": spacy_nlp(\"central indian restaurant\")\n",
    "        })]\n",
    "\n",
    "    # uses BILOU and the default features\n",
    "    ext.train(TrainingData(training_examples=examples), RasaNLUModelConfig())\n",
    "    sentence = 'anywhere in the west'\n",
    "    doc = {\"spacy_doc\": spacy_nlp(sentence)}\n",
    "    crf_format = ext._from_text_to_crf(Message(sentence, doc))\n",
    "    assert [word[0] for word in crf_format] == ['anywhere', 'in', 'the', 'west']\n",
    "    feats = ext._sentence_to_features(crf_format)\n",
    "    assert 'BOS' in feats[0]\n",
    "    assert 'EOS' in feats[-1]\n",
    "    assert feats[1]['0:low'] == \"in\"\n",
    "    sentence = 'anywhere in the west'\n",
    "    ext.extract_entities(Message(sentence, {\"spacy_doc\": spacy_nlp(sentence)}))\n",
    "    filtered = ext.filter_trainable_entities(examples)\n",
    "    assert filtered[0].get('entities') == [\n",
    "        {\"start\": 16, \"end\": 20, \"value\": \"west\", \"entity\": \"location\"}\n",
    "    ], 'Entity without extractor remains'\n",
    "    assert filtered[1].get('entities') == [\n",
    "        {\"start\": 8, \"end\": 14,\n",
    "         \"value\": \"indian\", \"entity\": \"cuisine\", \"extractor\": \"ner_crf\"}\n",
    "    ], 'Only ner_crf entity annotation remains'\n",
    "    assert examples[1].get('entities')[0] == {\n",
    "        \"start\": 0, \"end\": 7,\n",
    "        \"value\": \"central\", \"entity\": \"location\",\n",
    "        \"extractor\": \"random_extractor\"\n",
    "    }, 'Original examples are not mutated'\n",
    "\n",
    "test_crf_extractor(spacy_nlp, ner_crf_pos_feature_config())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-26T11:08:39.636905Z",
     "start_time": "2018-12-26T11:08:12.485066Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:rasa_nlu.config:Tried to set configuration value for component 'ner_duckling' which is not part of the pipeline.\n",
      "DEBUG:root:Passing reference time 2013-10-12T00:03:02+00:00 to duckling\n"
     ]
    }
   ],
   "source": [
    "def test_duckling_entity_extractor(component_builder):\n",
    "    _config = RasaNLUModelConfig({\"pipeline\": [{\"name\": \"ner_duckling\"}]})\n",
    "    _config.set_component_attr(\"ner_duckling\", dimensions=[\"time\"])\n",
    "    duckling = component_builder.create_component(\"ner_duckling\", _config)\n",
    "    message = Message(\"Today is the 5th of May. Let us meet tomorrow.\")\n",
    "    duckling.process(message)\n",
    "    entities = message.get(\"entities\")\n",
    "    assert len(entities) == 3\n",
    "\n",
    "    # Test duckling with a defined date\n",
    "\n",
    "    # 1381536182000 == 2013/10/12 02:03:02\n",
    "    message = Message(\"Let us meet tomorrow.\", time=\"1381536182000\")\n",
    "    duckling.process(message)\n",
    "    entities = message.get(\"entities\")\n",
    "    assert len(entities) == 1\n",
    "    assert entities[0][\"text\"] == \"tomorrow\"\n",
    "    assert entities[0][\"value\"] == \"2013-10-13T00:00:00.000Z\"\n",
    "\n",
    "\n",
    "def test_duckling_entity_extractor_and_synonyms(component_builder):\n",
    "    _config = RasaNLUModelConfig({\"pipeline\": [{\"name\": \"ner_duckling\"}]})\n",
    "    _config.set_component_attr(\"ner_duckling\", dimensions=[\"number\"])\n",
    "    duckling = component_builder.create_component(\"ner_duckling\", _config)\n",
    "    synonyms = component_builder.create_component(\"ner_synonyms\", _config)\n",
    "    message = Message(\"He was 6 feet away\")\n",
    "    duckling.process(message)\n",
    "    # checks that the synonym processor can handle entities that have int values\n",
    "    synonyms.process(message)\n",
    "    assert message is not None\n",
    "\n",
    "test_duckling_entity_extractor(component_builder())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-02T13:07:18.512042Z",
     "start_time": "2019-01-02T13:07:18.164139Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['お', '皿', 'を', '二', '枚', 'ください', '。']\n",
      "[0, 1, 2, 3, 4, 5, 9]\n"
     ]
    }
   ],
   "source": [
    "from sagas.ja.japanese_tokenizer import JapaneseTokenizer\n",
    "tk = JapaneseTokenizer()\n",
    "print([t.text for t in tk.tokenize(\"お皿を二枚ください。\")])\n",
    "print([t.offset for t in tk.tokenize(\"お皿を二枚ください。\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-26T11:00:19.527379Z",
     "start_time": "2018-12-26T11:00:19.521508Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['我', '想', '去', '吃', '兰州', '拉面']\n"
     ]
    }
   ],
   "source": [
    "from rasa_nlu.tokenizers.jieba_tokenizer import JiebaTokenizer\n",
    "tk = JiebaTokenizer()\n",
    "print([t.text for t in tk.tokenize(\"我想去吃兰州拉面\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
