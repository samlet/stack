{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-03T08:04:06.821284Z",
     "start_time": "2019-02-03T08:04:06.340499Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26\n"
     ]
    }
   ],
   "source": [
    "import pyarrow as pa\n",
    "data = b'abcdefghijklmnopqrstuvwxyz'\n",
    "buf = pa.py_buffer(data)\n",
    "print(buf.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-03T08:04:55.832274Z",
     "start_time": "2019-02-03T08:04:55.826388Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'abcdefghijklmnopqrstuvwxyz'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memoryview(buf)\n",
    "buf.to_pybytes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-03T08:05:33.678443Z",
     "start_time": "2019-02-03T08:05:33.670787Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'some'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "buf = memoryview(b\"some data\")\n",
    "stream = pa.input_stream(buf)\n",
    "stream.read(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-03T08:06:49.204626Z",
     "start_time": "2019-02-03T08:06:49.200381Z"
    }
   },
   "outputs": [],
   "source": [
    "with open('./out/example2.dat', 'wb') as f:\n",
    "    f.write(b'some example data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-03T08:08:25.727442Z",
     "start_time": "2019-02-03T08:08:25.720521Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'some'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_obj = pa.OSFile('./out/example2.dat')\n",
    "file_obj.read(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-03T08:08:46.203535Z",
     "start_time": "2019-02-03T08:08:46.195031Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'some'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using pyarrow’s OSFile class, you can write:\n",
    "with pa.OSFile('./out/example3.dat', 'wb') as f:\n",
    "    f.write(b'some example data')\n",
    "mmap = pa.memory_map('./out/example3.dat')\n",
    "mmap.read(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-03T08:09:16.371876Z",
     "start_time": "2019-02-03T08:09:16.365335Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyarrow.lib.Buffer object at 0x114442618>\n"
     ]
    }
   ],
   "source": [
    "mmap.seek(0)\n",
    "buf = mmap.read_buffer(4)\n",
    "print(buf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-03T08:09:22.284586Z",
     "start_time": "2019-02-03T08:09:22.279283Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'some'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "buf.to_pybytes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In-Memory Reading and Writing\n",
    "To assist with serialization and deserialization of in-memory data, we have file interfaces that can read and write to Arrow Buffers.\n",
    "\n",
    "These have similar semantics to Python’s built-in io.BytesIO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-03T08:10:59.513230Z",
     "start_time": "2019-02-03T08:10:59.499019Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyarrow.lib.Buffer object at 0x114442688>\n",
      "14\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "b'friends'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "writer = pa.BufferOutputStream()\n",
    "writer.write(b'hello, friends')\n",
    "# 14\n",
    "\n",
    "buf = writer.getvalue()\n",
    "print(buf)\n",
    "# <pyarrow.lib.Buffer at 0x2b9df4d9d180>\n",
    "print(buf.size)\n",
    "# 14\n",
    "\n",
    "reader = pa.BufferReader(buf)\n",
    "reader.seek(7)\n",
    "reader.read(7)\n",
    "# b'friends'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Types and In-Memory Data Model\n",
    "Apache Arrow defines columnar array data structures by composing type metadata with memory buffers, like the ones explained in the documentation on Memory and IO. These data structures are exposed in Python through a series of interrelated classes:\n",
    "\n",
    "* Type Metadata: Instances of pyarrow.DataType, which describe a logical array type\n",
    "* Schemas: Instances of pyarrow.Schema, which describe a named collection of types. These can be thought of as the column types in a table-like object.\n",
    "* Arrays: Instances of pyarrow.Array, which are atomic, contiguous columnar data structures composed from Arrow Buffer objects\n",
    "* Record Batches: Instances of pyarrow.RecordBatch, which are a collection of Array objects with a particular Schema\n",
    "* Tables: Instances of pyarrow.Table, a logical table data structure in which each column consists of one or more pyarrow.Array objects of the same type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-03T08:14:32.029749Z",
     "start_time": "2019-02-03T08:14:32.022756Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "int32\n",
      "int32_field\n",
      "int32\n",
      "list<item: int32>\n"
     ]
    }
   ],
   "source": [
    "t1 = pa.int32()\n",
    "print(t1)\n",
    "f0 = pa.field('int32_field', t1)\n",
    "print(f0.name)\n",
    "print(f0.type)\n",
    "t6 = pa.list_(t1)\n",
    "print(t6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-03T08:15:28.891609Z",
     "start_time": "2019-02-03T08:15:28.880243Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "struct<s0: int32, s1: string, s2: fixed_size_binary[10], s3: list<item: int32>>\n"
     ]
    }
   ],
   "source": [
    "t1 = pa.int32()\n",
    "t2 = pa.string()\n",
    "t3 = pa.binary()\n",
    "t4 = pa.binary(10)\n",
    "t5 = pa.timestamp('ms')\n",
    "\n",
    "fields = [\n",
    "    pa.field('s0', t1),\n",
    "    pa.field('s1', t2),\n",
    "    pa.field('s2', t4),\n",
    "    pa.field('s3', t6),\n",
    "]\n",
    "\n",
    "t7 = pa.struct(fields)\n",
    "print(t7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-03T08:16:03.285844Z",
     "start_time": "2019-02-03T08:16:03.275404Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "field0: int32\n",
       "field1: string\n",
       "field2: fixed_size_binary[10]\n",
       "field3: list<item: int32>\n",
       "  child 0, item: int32"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_schema = pa.schema([('field0', t1),\n",
    "                       ('field1', t2),\n",
    "                       ('field2', t4),\n",
    "                       ('field3', t6)])\n",
    "\n",
    "my_schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-03T11:40:59.106056Z",
     "start_time": "2019-02-03T11:40:59.091390Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "4\n",
      "f0: int64\n",
      "f1: string\n",
      "f2: bool\n",
      "[\n",
      "  \"foo\",\n",
      "  \"bar\",\n",
      "  \"baz\",\n",
      "  null\n",
      "]\n",
      "[\n",
      "  \"bar\",\n",
      "  \"baz\",\n",
      "  null\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "data = [\n",
    "    pa.array([1, 2, 3, 4]),\n",
    "    pa.array(['foo', 'bar', 'baz', None]),\n",
    "    pa.array([True, None, False, True])\n",
    "]\n",
    "batch = pa.RecordBatch.from_arrays(data, ['f0', 'f1', 'f2'])\n",
    "print(batch.num_columns)\n",
    "print(batch.num_rows)\n",
    "print(batch.schema)\n",
    "print(batch[1])\n",
    "batch2 = batch.slice(1, 3)\n",
    "print(batch2[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 表\n",
    "PyArrow Table类型不是Apache Arrow规范的一部分，而是一个帮助将多个记录批次和数组片段作为单个逻辑数据集进行争论的工具。作为一个相关的例子，我们可能在套接字流中接收多个小记录批次，然后需要将它们连接成连续的内存以便在NumPy或pandas中使用。Table对象使这种方法更有效，而无需额外的内存复制。\n",
    "\n",
    "考虑到我们上面创建的记录批次，我们可以使用以下方法创建一个包含批次的一个或多个副本的表Table.from_batches："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-03T11:45:23.869071Z",
     "start_time": "2019-02-03T11:45:23.862342Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pyarrow.Table\n",
      "f0: int64\n",
      "f1: string\n",
      "f2: bool\n",
      "20\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "batches = [batch] * 5\n",
    "table = pa.Table.from_batches(batches)\n",
    "print(table)\n",
    "print(table.num_rows)\n",
    "print(table.num_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-03T11:46:08.546152Z",
     "start_time": "2019-02-03T11:46:08.539483Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Column name='f0' type=DataType(int64)>\n",
       "[\n",
       "  [\n",
       "    1,\n",
       "    2,\n",
       "    3,\n",
       "    4\n",
       "  ],\n",
       "  [\n",
       "    1,\n",
       "    2,\n",
       "    3,\n",
       "    4\n",
       "  ],\n",
       "  [\n",
       "    1,\n",
       "    2,\n",
       "    3,\n",
       "    4\n",
       "  ],\n",
       "  [\n",
       "    1,\n",
       "    2,\n",
       "    3,\n",
       "    4\n",
       "  ],\n",
       "  [\n",
       "    1,\n",
       "    2,\n",
       "    3,\n",
       "    4\n",
       "  ]\n",
       "]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 表的列是实例Column，它是一个或多个相同类型的数组的容器。\n",
    "c = table[0]\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-03T11:46:50.850698Z",
     "start_time": "2019-02-03T11:46:50.845386Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  [\n",
      "    1,\n",
      "    2,\n",
      "    3,\n",
      "    4\n",
      "  ],\n",
      "  [\n",
      "    1,\n",
      "    2,\n",
      "    3,\n",
      "    4\n",
      "  ],\n",
      "  [\n",
      "    1,\n",
      "    2,\n",
      "    3,\n",
      "    4\n",
      "  ],\n",
      "  [\n",
      "    1,\n",
      "    2,\n",
      "    3,\n",
      "    4\n",
      "  ],\n",
      "  [\n",
      "    1,\n",
      "    2,\n",
      "    3,\n",
      "    4\n",
      "  ]\n",
      "]\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "print(c.data)\n",
    "print(c.data.num_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-03T11:47:02.783674Z",
     "start_time": "2019-02-03T11:47:02.777238Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  1,\n",
      "  2,\n",
      "  3,\n",
      "  4\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "print(c.data.chunk(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-03T11:48:05.985783Z",
     "start_time": "2019-02-03T11:48:05.963487Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     1\n",
       "1     2\n",
       "2     3\n",
       "3     4\n",
       "4     1\n",
       "5     2\n",
       "6     3\n",
       "7     4\n",
       "8     1\n",
       "9     2\n",
       "10    3\n",
       "11    4\n",
       "12    1\n",
       "13    2\n",
       "14    3\n",
       "15    4\n",
       "16    1\n",
       "17    2\n",
       "18    3\n",
       "19    4\n",
       "Name: f0, dtype: int64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 可以将这些对象转换为连续的NumPy数组，以便在pandas中使用：\n",
    "c.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-03T11:48:45.683449Z",
     "start_time": "2019-02-03T11:48:45.673222Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pyarrow.concat_tables如果模式相同，也可以将多个表连接在一起以形成单个表 ：\n",
    "tables = [table] * 2\n",
    "table_all = pa.concat_tables(tables)\n",
    "table_all.num_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-03T11:49:15.292996Z",
     "start_time": "2019-02-03T11:49:15.286523Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = table_all[0]\n",
    "c.data.num_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-03T19:22:37.518570Z",
     "start_time": "2019-02-03T19:22:37.501367Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f0</th>\n",
       "      <th>f1</th>\n",
       "      <th>f2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>foo</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>bar</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>baz</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>foo</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   f0    f1     f2\n",
       "0   1   foo   True\n",
       "1   2   bar   None\n",
       "2   3   baz  False\n",
       "3   4  None   True\n",
       "4   1   foo   True"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_new = table_all.to_pandas()\n",
    "df_new[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这类似于Table.from_batches，但使用表作为输入而不是记录批次。记录批次可以制作成表格，但不是相反，所以如果您的数据已经是表格形式，那么请使用 pyarrow.concat_tables。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arrow定义了两种用于序列化记录批次的二进制格式：\n",
    "\n",
    "- 流格式：用于发送任意长度的记录批次序列。必须从头到尾处理格式，并且不支持随机访问\n",
    "- 文件或随机访问格式：用于序列化固定数量的记录批次。支持随机访问，因此在与内存映射一起使用时非常有用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-03T11:53:03.066730Z",
     "start_time": "2019-02-03T11:53:03.056678Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyarrow as pa\n",
    "\n",
    "data = [\n",
    "    pa.array([1, 2, 3, 4]),\n",
    "    pa.array(['foo', 'bar', 'baz', None]),\n",
    "    pa.array([True, None, False, True])\n",
    "]\n",
    "\n",
    "batch = pa.RecordBatch.from_arrays(data, ['f0', 'f1', 'f2'])\n",
    "batch.num_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-03T11:55:18.799115Z",
     "start_time": "2019-02-03T11:55:18.786437Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1972\n",
      "<class 'pyarrow.lib.Buffer'>\n"
     ]
    }
   ],
   "source": [
    "# 现在，我们可以开始编写包含一些批次的流。为此我们使用RecordBatchStreamWriter，\n",
    "# 它可以写入可写 NativeFile对象或可写Python对象：\n",
    "# 这里我们使用了内存中的Arrow缓冲流，但这可能是一个套接字或其他一些IO接收器。\n",
    "sink = pa.BufferOutputStream()\n",
    "writer = pa.RecordBatchStreamWriter(sink, batch.schema)\n",
    "\n",
    "for i in range(5):\n",
    "    writer.write_batch(batch)\n",
    "\n",
    "writer.close()\n",
    "buf = sink.getvalue()\n",
    "print(buf.size)\n",
    "print(type(buf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-03T11:55:51.719737Z",
     "start_time": "2019-02-03T11:55:51.708289Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f0: int64\n",
      "f1: string\n",
      "f2: bool\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reader = pa.ipc.open_stream(buf)\n",
    "print(reader.schema)\n",
    "\n",
    "batches = [b for b in reader]\n",
    "len(batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-03T11:56:10.633802Z",
     "start_time": "2019-02-03T11:56:10.626341Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 检查返回的批次是否与原始输入相同：\n",
    "batches[0].equals(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-03T11:57:27.296263Z",
     "start_time": "2019-02-03T11:57:27.284084Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4210"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 在RecordBatchFileWriter具有相同的API RecordBatchStreamWriter：\n",
    "sink = pa.BufferOutputStream()\n",
    "writer = pa.RecordBatchFileWriter(sink, batch.schema)\n",
    "\n",
    "for i in range(10):\n",
    "    writer.write_batch(batch)\n",
    "\n",
    "writer.close()\n",
    "\n",
    "buf = sink.getvalue()\n",
    "buf.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RecordBatchFileReader和 之间的区别在于RecordBatchStreamReader输入源必须具有seek随机访问的 方法。流读取器仅需要读取操作。我们也可以使用该pyarrow.ipc.open_file方法打开一个文件："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-03T11:58:43.788120Z",
     "start_time": "2019-02-03T11:58:43.783955Z"
    }
   },
   "outputs": [],
   "source": [
    "reader = pa.ipc.open_file(buf)\n",
    "reader.num_record_batches\n",
    "# 10\n",
    "b = reader.get_batch(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-03T12:01:19.333708Z",
     "start_time": "2019-02-03T12:01:19.320553Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f0</th>\n",
       "      <th>f1</th>\n",
       "      <th>f2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>foo</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>bar</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>baz</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>foo</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   f0    f1     f2\n",
       "0   1   foo   True\n",
       "1   2   bar   None\n",
       "2   3   baz  False\n",
       "3   4  None   True\n",
       "4   1   foo   True"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 流和文件阅读器类有一个特殊的read_pandas方法来简化读取多个记录批次并将它们转换为单个DataFrame输出\n",
    "df = pa.ipc.open_file(buf).read_pandas()\n",
    "df[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 任意对象序列化\n",
    "在pyarrow我们能够序列化和反序列化多种Python对象。虽然不是pickle模块的完全替代品，但这些功能可以明显更快，特别是在处理NumPy阵列的集合时。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-03T12:21:43.823519Z",
     "start_time": "2019-02-03T12:21:42.922483Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "data = {\n",
    "    i: np.random.randn(500, 500)\n",
    "    for i in range(100)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-03T12:25:00.830386Z",
     "start_time": "2019-02-03T12:25:00.609661Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyarrow.lib.Buffer'>\n",
      "200028864\n"
     ]
    }
   ],
   "source": [
    "buf = pa.serialize(data).to_buffer()\n",
    "print(type(buf))\n",
    "print(buf.size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pyarrow.serialize创建一个中间对象，可以将其转换为缓冲区（to_buffer方法）或直接写入输出流。\n",
    "pyarrow.deserialize 将类缓冲区对象转换回原始Python对象：\n",
    "处理NumPy数组时，pyarrow.deserialize可能会明显快于pickle因为生成的数组是对输入缓冲区的零拷贝引用。阵列越大，性能节省越多。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-03T12:26:06.630205Z",
     "start_time": "2019-02-03T12:26:06.621557Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.12742827, -0.95459129,  0.05318619, ..., -0.74436129,\n",
       "         0.96213285, -0.42219318],\n",
       "       [-2.0200782 , -1.33166671, -1.49418935, ...,  0.29310029,\n",
       "        -0.95510217, -0.31432893],\n",
       "       [-0.25230722, -0.24753379, -2.61528964, ...,  0.70687883,\n",
       "         0.68887424, -0.27028108],\n",
       "       ...,\n",
       "       [ 1.4241211 , -0.44546557, -0.11266728, ..., -0.07963118,\n",
       "        -0.41794279,  0.60465669],\n",
       "       [ 0.55870002,  0.90073452, -0.34766403, ...,  0.15999091,\n",
       "         1.78429767, -1.32387707],\n",
       "       [-1.84781187,  0.80243619,  0.31178591, ..., -0.12645207,\n",
       "         0.38196331, -0.13160539]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "restored_data = pa.deserialize(buf)\n",
    "restored_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-03T12:26:24.607767Z",
     "start_time": "2019-02-03T12:26:20.428205Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "509 µs ± 14.7 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit restored_data = pa.deserialize(buf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-03T12:27:08.393762Z",
     "start_time": "2019-02-03T12:27:06.528413Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133 ms ± 8.41 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "pickled = pickle.dumps(data)\n",
    "%timeit unpickled_data = pickle.loads(pickled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-03T12:33:18.087826Z",
     "start_time": "2019-02-03T12:33:18.075752Z"
    }
   },
   "outputs": [],
   "source": [
    "import pyarrow as pa\n",
    "\n",
    "# 考虑一个有两个成员的类，其中一个是NumPy数组：\n",
    "class MyData:\n",
    "    def __init__(self, name, data):\n",
    "        self.name = name\n",
    "        self.data = data\n",
    "\n",
    "# 我们编写函数来将它转换为具有更简单类型的字典：\n",
    "def _serialize_MyData(val):\n",
    "    return {'name': val.name, 'data': val.data}\n",
    "\n",
    "def _deserialize_MyData(data):\n",
    "    return MyData(data['name'], data['data'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-03T12:35:24.886480Z",
     "start_time": "2019-02-03T12:35:24.873084Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.MyData object at 0x10f9f47b8> world\n"
     ]
    }
   ],
   "source": [
    "# 那么，我们必须在注册这些功能SerializationContext，这样 MyData可以确认：\n",
    "context=pa.SerializationContext()\n",
    "context.register_type(MyData, 'MyData',\n",
    "                      custom_serializer=_serialize_MyData,\n",
    "                      custom_deserializer=_deserialize_MyData)\n",
    "\n",
    "val=MyData(\"hello\", \"world\")\n",
    "# 最后，我们使用此上下文作为附加参数pyarrow.serialize：\n",
    "buf = pa.serialize(val, context=context).to_buffer()\n",
    "restored_val = pa.deserialize(buf, context=context)\n",
    "# 该SerializationContext还具有方便的方法serialize和 deserialize，所以这些都是等效的声明：\n",
    "buf = context.serialize(val).to_buffer()\n",
    "restored_val = context.deserialize(buf)\n",
    "print(restored_val, restored_val.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 基于组件的序列化\n",
    "对于序列化包含一定数量的NumPy数组，箭头缓冲区或其他数据类型的Python对象，可能需要传输其序列化表示而不必使用该to_buffer方法生成中间副本 。为了激发这一点，假设我们有一个NumPy数组列表：..\n",
    "\n",
    "该调用pa.serialize(data)不会复制每个NumPy数组中的内存。然后，可以将此序列化表示分解为包含一系列pyarrow.Buffer对象的字典，该对象包含每个数组的元数据以及对数组内部内存的引用。为此，请使用以下to_components方法："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-03T12:48:37.707509Z",
     "start_time": "2019-02-03T12:48:37.667054Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "data = [np.random.randn(10, 10) for i in range(5)]\n",
    "\n",
    "serialized = pa.serialize(data)\n",
    "components = serialized.to_components()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-03T12:48:41.938087Z",
     "start_time": "2019-02-03T12:48:41.930838Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<memory at 0x10f95cf48>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memoryview(components['data'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-03T12:49:18.785551Z",
     "start_time": "2019-02-03T12:49:18.781742Z"
    }
   },
   "outputs": [],
   "source": [
    "# memoryview可以转换回箭Buffer与 pyarrow.py_buffer：\n",
    "mv = memoryview(components['data'][0])\n",
    "buf = pa.py_buffer(mv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-03T12:49:49.625618Z",
     "start_time": "2019-02-03T12:49:49.616269Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.50122557, -1.19676195,  0.1266847 , -1.85551048, -0.86232847,\n",
       "        -1.36439755,  0.32611734, -1.38306278, -0.99390007,  1.39160018],\n",
       "       [ 1.61327357, -0.25970142, -0.20182893,  0.08072281,  0.60007608,\n",
       "         0.28938348,  1.2834959 , -0.63898964, -1.47821378,  1.56514748],\n",
       "       [ 0.92313537, -1.14870166,  2.60787796,  0.95204378, -1.54519512,\n",
       "        -1.02754663,  1.79460792,  0.53590907,  0.39266188, -0.57042973],\n",
       "       [-1.93516557,  1.049958  ,  0.61075164,  1.61545997,  0.22881206,\n",
       "         1.26159629,  1.87400733, -1.20716317, -1.80240749, -0.22434756],\n",
       "       [ 0.16278112, -0.7571062 ,  1.10146151,  0.13153447, -1.04254708,\n",
       "         1.00061581, -1.07659186,  0.14857146, -0.24341449, -1.09934827],\n",
       "       [ 2.5084868 ,  1.81081776, -0.01036504,  2.24718117,  0.51473965,\n",
       "        -0.46864375,  0.09480174,  0.4545933 ,  0.82106186,  0.50150186],\n",
       "       [ 0.48789061,  0.54642272,  0.72467134,  1.47021037, -1.93137476,\n",
       "        -0.22946273,  3.05576351, -0.3826343 , -1.15538994, -0.07947432],\n",
       "       [ 1.01562484, -0.52597014, -0.18405145,  0.36576734, -1.48705953,\n",
       "        -1.21893525,  0.16605511, -2.62251042,  0.83893348, -0.83608006],\n",
       "       [ 1.25871281, -2.42671317,  1.40415403,  0.57380619, -0.21489983,\n",
       "         0.24682323,  0.76804858,  0.33246645,  0.64012739,  0.48376685],\n",
       "       [-0.65760164,  0.54717607,  1.00240976, -0.18590532, -0.97579216,\n",
       "        -1.42196991, -0.34964825,  0.82587932,  0.08429002,  1.78519871]])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 使用以下方法从基于组件的表示重建对象 deserialize_components：\n",
    "restored_data = pa.deserialize_components(components)\n",
    "restored_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 序列化pandas对象\n",
    "默认序列化方面进行了优化像熊猫的对象处理DataFrame和Series。结合上面的基于组件的序列化，这可以实现不包含任何Python对象的pandas DataFrame对象的零拷贝传输："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-03T18:43:59.550233Z",
     "start_time": "2019-02-03T18:43:59.509085Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   a\n",
       "0  1\n",
       "1  2\n",
       "2  3\n",
       "3  4\n",
       "4  5"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame({'a': [1, 2, 3, 4, 5]})\n",
    "context = pa.default_serialization_context()\n",
    "\n",
    "serialized_df = context.serialize(df)\n",
    "df_components = serialized_df.to_components()\n",
    "original_df = context.deserialize_components(df_components)\n",
    "original_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
