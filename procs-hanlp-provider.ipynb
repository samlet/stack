{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-02T08:38:08.638997Z",
     "start_time": "2019-01-02T08:38:05.695426Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "这里 r 2\n",
      "是 v 1\n",
      "北京 ns 2\n",
      "rzs 这里 0 2\n",
      "vshi 是 2 3\n",
      "ns 北京 3 5\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "from google.protobuf.json_format import MessageToJson\n",
    "from client_wrapper import ServiceClient\n",
    "\n",
    "import nlpserv_pb2 as nlp_messages\n",
    "import nlpserv_pb2_grpc as nlp_service\n",
    "from utils import dump\n",
    "\n",
    "client=ServiceClient(nlp_service, 'NlpProcsStub', 'localhost', 10052)\n",
    "\n",
    "def tokenize(text):\n",
    "    request = nlp_messages.NlTokenizerRequest(text=nlp_messages.NlText(text=text))\n",
    "    response = client.Tokenizer(request)\n",
    "    return response\n",
    "def extract(text):\n",
    "    request = nlp_messages.NlTokenizerRequest(text=nlp_messages.NlText(text=text))\n",
    "    response = client.EntityExtractor(request)\n",
    "    return response\n",
    "\n",
    "# print(response)\n",
    "response=tokenize(\"这里是北京\")\n",
    "for t in response.tokens:\n",
    "    # print(MessageToJson(resp))\n",
    "    print(t.text, t.label, t.length)\n",
    "\n",
    "extract_r=extract(\"这里是北京\")\n",
    "for t in extract_r.entities:\n",
    "    # print(MessageToJson(resp))\n",
    "    print(t.entity, t.value, t.start, t.end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-02T09:33:35.991490Z",
     "start_time": "2019-01-02T09:33:35.965102Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'nlpserv_pb2.NlAmountList'>\n",
      "19 0 2\n"
     ]
    }
   ],
   "source": [
    "text=\"十九元套餐包括什么\"\n",
    "request = nlp_messages.NlText(text=text)\n",
    "response = client.ParseAmountTerms(request)\n",
    "print(type(response))\n",
    "for t in response.amount:\n",
    "    print(t.numericVal, t.entity.start, t.entity.end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-27T12:04:38.929359Z",
     "start_time": "2018-12-27T12:04:38.907844Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "这里 0\n",
      "是 2\n",
      "北京 3\n"
     ]
    }
   ],
   "source": [
    "from rasa_nlu.tokenizers import Tokenizer, Token\n",
    "\n",
    "def tokenize_msg(text, msg_tokens):         \n",
    "    words=[]\n",
    "    for token in msg_tokens.tokens:\n",
    "        words.append(token.text)\n",
    "\n",
    "    running_offset = 0\n",
    "    tokens = []\n",
    "    for word in words:\n",
    "        word_offset = text.index(word, running_offset)\n",
    "        word_len = len(word)\n",
    "        running_offset = word_offset + word_len\n",
    "        tokens.append(Token(word, word_offset))   \n",
    "    return tokens\n",
    "\n",
    "tokens=tokenize_msg(\"这里是北京\", response)\n",
    "for t in tokens:\n",
    "    print(t.text, t.offset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-27T14:17:39.023151Z",
     "start_time": "2018-12-27T14:17:38.993517Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "这里 0\n",
      "是 2\n",
      "北京 3\n"
     ]
    }
   ],
   "source": [
    "from rasa_nlu.tokenizers import Tokenizer, Token\n",
    "\n",
    "def tokenize_msg(text, msg_tokens):         \n",
    "    words=[]\n",
    "    for token in msg_tokens.entities:\n",
    "        words.append(token.value)\n",
    "\n",
    "    running_offset = 0\n",
    "    tokens = []\n",
    "    for word in words:\n",
    "        word_offset = text.index(word, running_offset)\n",
    "        word_len = len(word)\n",
    "        running_offset = word_offset + word_len\n",
    "        tokens.append(Token(word, word_offset))   \n",
    "    return tokens\n",
    "\n",
    "text=\"这里是北京\"\n",
    "tokens=tokenize_msg(text, extract(text))\n",
    "for t in tokens:\n",
    "    print(t.text, t.offset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-27T13:05:55.852259Z",
     "start_time": "2018-12-27T13:05:55.834892Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'confidence': None, 'end': 5, 'entity': 'ns', 'start': 3, 'value': '北京'}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extract_entities(text, msg_tokens, filters):\n",
    "    running_offset = 0\n",
    "    entities = []\n",
    "    for token in msg_tokens.tokens:\n",
    "        word=token.text\n",
    "        word_offset = text.index(word, running_offset)\n",
    "        word_len = len(word)\n",
    "        running_offset = word_offset + word_len\n",
    "        if token.label in filters:\n",
    "            entities.append({\n",
    "                    \"entity\": token.label,\n",
    "                    \"value\": token.text,\n",
    "                    \"start\": word_offset,\n",
    "                    \"confidence\": None,\n",
    "                    \"end\": running_offset\n",
    "                })\n",
    "    return entities\n",
    "\n",
    "extract_entities(\"这里是北京\", response, [\"ns\", \"nr\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-27T14:20:45.506503Z",
     "start_time": "2018-12-27T14:20:45.460818Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'confidence': None, 'end': 5, 'entity': 'ns', 'start': 3, 'value': '北京'}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extract_entities(text, msg_tokens, filters):\n",
    "    running_offset = 0\n",
    "    entities = []\n",
    "    for token in msg_tokens.entities:     \n",
    "        if token.entity in filters:\n",
    "            entities.append({\n",
    "                    \"entity\": token.entity,\n",
    "                    \"value\": token.value,\n",
    "                    \"start\": token.start,\n",
    "                    \"confidence\": None,\n",
    "                    \"end\": token.end\n",
    "                })\n",
    "    return entities\n",
    "\n",
    "text=\"这里是北京\"\n",
    "extract_entities(text, extract(text), [\"ns\", \"nr\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-27T14:22:25.180266Z",
     "start_time": "2018-12-27T14:22:25.147131Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "person 张晚霞\n",
      "location 香港\n",
      "organization 微软公司\n",
      "proper 於\n",
      "amount 1975年\n",
      "proper 艾\n"
     ]
    }
   ],
   "source": [
    "# nz\t其他专名: 除人名、国名、地名、团体、机构、组织以外的其他专有名词都标以nz。满族/nz，俄罗斯族/nz，汉语/nz，罗马利亚语/nz， 捷克语/nz，中文/nz， 英文/nz， 满人/nz， 哈萨克人/nz， 诺贝尔奖/nz， 茅盾奖/nz， 1.包含专有名称（或简称）的交通线，标以nz；短语型的，标为NZ。津浦路/nz， 石太线/nz， [京/j 九/j 铁路/n]NZ， [京/j 津/j 高速/b 公路/n]NZ， 2. 历史上重要事件、运动等专有名称一般是短语型的，按短语型专有名称处理，标以NZ。[卢沟桥/ns 事件/n]NZ， [西安/ns 事变/n]NZ，[五四/t 运动/n]NZ， [明治/nz 维新/n]NZ，[甲午/t 战争/n]NZ，3.专有名称后接多音节的名词，如“语言”、“文学”、“文化”、“方式”、“精神”等，失去专指性，则应分开。欧洲/ns 语言/n， 法国/ns 文学/n， 西方/ns 文化/n， 贝多芬/nr 交响乐/n， 雷锋/nr 精神/n， 美国/ns 方式/n，日本/ns 料理/n， 宋朝/t 古董/n 4. 商标（包括专名及后接的“牌”、“型”等）是专指的，标以nz，但其后所接的商品仍标以普通名词n。康师傅/nr 方便面/n， 中华牌/nz 香烟/n， 牡丹III型/nz 电视机/n， 联想/nz 电脑/n， 鳄鱼/nz 衬衣/n， 耐克/nz 鞋/n5. 以序号命名的名称一般不认为是专有名称。2/m 号/q 国道/n ，十一/m 届/q 三中全会/j如果前面有专名，合起来作为短语型专名。[中国/ns 101/m 国道/n]NZ， [中共/j 十一/m 届/q 三中全会/j]NZ，6. 书、报、杂志、文档、报告、协议、合同等的名称通常有书名号加以标识，不作为专有名词。由于这些名字往往较长，名字本身按常规处理。《/w 宁波/ns 日报/n 》/w ，《/w 鲁迅/nr 全集/n 》/w，中华/nz 读书/vn 报/n， 杜甫/nr 诗选/n，少数书名、报刊名等专有名称，则不切分。红楼梦/nz， 人民日报/nz，儒林外史/nz 7. 当有些专名无法分辨它们是人名还是地名或机构名时，暂标以nz。[巴黎/ns 贝尔希/nz 体育馆/n]NT，其中“贝尔希”只好暂标为nz。\n",
    "mappings={\"ns\":\"location\", \n",
    "          \"nr\": \"person\", \n",
    "          \"nt\": \"organization\",\n",
    "          \"nz\":\"proper\",\n",
    "          \"nx\":\"foreign\",\n",
    "          \"s\":\"space\",\n",
    "          \"t\":\"time\",\n",
    "          \"mq\":\"amount\",\n",
    "          \"m\":\"numeral\",\n",
    "          \"nf\":\"food\"\n",
    "         }\n",
    "def parse_print(text, filters):\n",
    "    response=extract(text)\n",
    "    result=extract_entities(text, response, filters)\n",
    "    for en in result:\n",
    "        print(mappings[en[\"entity\"]], en.get(\"value\"))\n",
    "\n",
    "filters=mappings.keys()\n",
    "parse_print(\"我的希望是希望张晚霞的背影被晚霞映红\", [\"ns\", \"nr\", \"nt\", \"s\"])\n",
    "parse_print(\"支援臺灣正體香港繁體：微软公司於1975年由比爾·蓋茲和保羅·艾倫創立。\", filters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-27T14:33:46.486962Z",
     "start_time": "2018-12-27T14:33:46.459688Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "person 蓝翔\n",
      "location 宁夏\n",
      "location 固原市\n",
      "location 彭阳县\n",
      "location 红河镇\n",
      "location 黑牛沟村\n",
      "蓝翔 v 2\n",
      "给 v 1\n",
      "宁夏 ns 2\n",
      "固原市 ns 3\n",
      "彭阳县 ns 3\n",
      "红河镇 ns 3\n",
      "黑牛沟村 ns 4\n",
      "捐赠 v 2\n",
      "了 u 1\n",
      "挖掘机 n 3\n"
     ]
    }
   ],
   "source": [
    "def tokenize_print(text):\n",
    "    response=tokenize(text)\n",
    "    for t in response.tokens:\n",
    "        # print(MessageToJson(resp))\n",
    "        print(t.text, t.label, t.length)\n",
    "\n",
    "text=\"蓝翔给宁夏固原市彭阳县红河镇黑牛沟村捐赠了挖掘机\"\n",
    "parse_print(text, filters)\n",
    "tokenize_print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-27T15:02:46.345582Z",
     "start_time": "2018-12-27T15:02:46.193831Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amount 十九元\n",
      "~~~~\n",
      "amount 九千九百九十九朵\n",
      "~~~~\n",
      "amount 壹佰块\n",
      "~~~~\n",
      "amount ９０１２３４５６７８只\n",
      "~~~~\n",
      "organization 牛奶三〇〇克*\n",
      "numeral 2\n",
      "~~~~\n",
      "foreign ChinaJoy\n",
      "amount 2厘米\n",
      "~~~~\n",
      "proper 要买\n",
      "numeral 五\n",
      "food 矿泉水\n",
      "~~~~\n"
     ]
    }
   ],
   "source": [
    "sents=[\n",
    "        \"十九元套餐包括什么\",\n",
    "        \"九千九百九十九朵玫瑰\",\n",
    "        \"壹佰块都不给我\",\n",
    "        \"９０１２３４５６７８只蚂蚁\",\n",
    "        \"牛奶三〇〇克*2\",\n",
    "        \"ChinaJoy“扫黄”细则露胸超2厘米罚款\",\n",
    "        \"我要买五瓶矿泉水\"\n",
    "]\n",
    "for text in sents:\n",
    "    parse_print(text, filters)\n",
    "    print(\"~~~~\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-27T15:02:15.850112Z",
     "start_time": "2018-12-27T15:02:13.793737Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amount 十九元 0\n",
      "n 套餐 3\n",
      "v 包括 5\n",
      "ry 什么 7\n",
      "~~~~\n",
      "amount 九千九百九十九朵 0\n",
      "n 玫瑰 8\n",
      "~~~~\n",
      "amount 壹佰块 0\n",
      "d 都 3\n",
      "d 不 4\n",
      "p 给 5\n",
      "rr 我 6\n",
      "~~~~\n",
      "amount ９０１２３４５６７８只 0\n",
      "n 蚂蚁 11\n",
      "~~~~\n",
      "organization 牛奶三〇〇克* 0\n",
      "numeral 2 7\n",
      "~~~~\n",
      "foreign ChinaJoy 0\n",
      "w “ 8\n",
      "vi 扫黄 9\n",
      "w ” 11\n",
      "n 细则 12\n",
      "v 露 14\n",
      "ng 胸 15\n",
      "v 超 16\n",
      "amount 2厘米 17\n",
      "vi 罚款 20\n",
      "~~~~\n",
      "rr 我 0\n",
      "proper 要买 1\n",
      "numeral 五 3\n",
      "n 瓶 4\n",
      "food 矿泉水 5\n",
      "~~~~\n"
     ]
    }
   ],
   "source": [
    "def extract_print(text):\n",
    "    response=extract(text)\n",
    "    for t in response.entities:\n",
    "        # print(MessageToJson(resp))\n",
    "        kind=t.entity\n",
    "        if t.entity in mappings:\n",
    "            kind=mappings.get(kind)\n",
    "        print(kind, t.value, t.start)\n",
    "for text in sents:\n",
    "    extract_print(text)        \n",
    "    print(\"~~~~\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
