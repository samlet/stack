{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-28T02:42:14.085187Z",
     "start_time": "2019-05-28T02:42:10.779120Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use device: cpu\n",
      "---\n",
      "Loading: tokenize\n",
      "With settings: \n",
      "{'model_path': '/pi/ai/corenlp/fr_gsd_models/fr_gsd_tokenizer.pt', 'lang': 'fr', 'shorthand': 'fr_gsd', 'mode': 'predict'}\n",
      "---\n",
      "Loading: mwt\n",
      "With settings: \n",
      "{'model_path': '/pi/ai/corenlp/fr_gsd_models/fr_gsd_mwt_expander.pt', 'lang': 'fr', 'shorthand': 'fr_gsd', 'mode': 'predict'}\n",
      "Building an attentional Seq2Seq model...\n",
      "Using a Bi-LSTM encoder\n",
      "Using soft attention for LSTM.\n",
      "Finetune all embeddings.\n",
      "---\n",
      "Loading: pos\n",
      "With settings: \n",
      "{'model_path': '/pi/ai/corenlp/fr_gsd_models/fr_gsd_tagger.pt', 'pretrain_path': '/pi/ai/corenlp/fr_gsd_models/fr_gsd.pretrain.pt', 'lang': 'fr', 'shorthand': 'fr_gsd', 'mode': 'predict'}\n",
      "---\n",
      "Loading: lemma\n",
      "With settings: \n",
      "{'model_path': '/pi/ai/corenlp/fr_gsd_models/fr_gsd_lemmatizer.pt', 'lang': 'fr', 'shorthand': 'fr_gsd', 'mode': 'predict'}\n",
      "Building an attentional Seq2Seq model...\n",
      "Using a Bi-LSTM encoder\n",
      "Using soft attention for LSTM.\n",
      "Finetune all embeddings.\n",
      "[Running seq2seq lemmatizer with edit classifier]\n",
      "---\n",
      "Loading: depparse\n",
      "With settings: \n",
      "{'model_path': '/pi/ai/corenlp/fr_gsd_models/fr_gsd_parser.pt', 'pretrain_path': '/pi/ai/corenlp/fr_gsd_models/fr_gsd.pretrain.pt', 'lang': 'fr', 'shorthand': 'fr_gsd', 'mode': 'predict'}\n",
      "Done loading processors!\n",
      "---\n",
      "<Token index=1;words=[<Word index=1;text=Van;lemma=Van;upos=PROPN;xpos=_;feats=_;governor=3;dependency_relation=nsubj>]>\n",
      "<Token index=2;words=[<Word index=2;text=Gogh;lemma=Gogh;upos=PROPN;xpos=_;feats=_;governor=1;dependency_relation=flat:name>]>\n",
      "<Token index=3;words=[<Word index=3;text=grandit;lemma=grandir;upos=VERB;xpos=_;feats=Mood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin;governor=0;dependency_relation=root>]>\n",
      "<Token index=4-5;words=[<Word index=4;text=à;lemma=à;upos=ADP;xpos=_;feats=_;governor=6;dependency_relation=case>, <Word index=5;text=le;lemma=le;upos=DET;xpos=_;feats=Definite=Def|Gender=Masc|Number=Sing|PronType=Art;governor=6;dependency_relation=det>]>\n",
      "<Token index=6;words=[<Word index=6;text=sein;lemma=sein;upos=NOUN;xpos=_;feats=Gender=Masc|Number=Sing;governor=3;dependency_relation=obl>]>\n",
      "<Token index=7;words=[<Word index=7;text=d';lemma=de;upos=ADP;xpos=_;feats=_;governor=9;dependency_relation=case>]>\n",
      "<Token index=8;words=[<Word index=8;text=une;lemma=un;upos=DET;xpos=_;feats=Definite=Ind|Gender=Fem|Number=Sing|PronType=Art;governor=9;dependency_relation=det>]>\n",
      "<Token index=9;words=[<Word index=9;text=famille;lemma=famille;upos=NOUN;xpos=_;feats=Gender=Fem|Number=Sing;governor=6;dependency_relation=nmod>]>\n",
      "<Token index=10;words=[<Word index=10;text=de;lemma=de;upos=ADP;xpos=_;feats=_;governor=13;dependency_relation=case>]>\n",
      "<Token index=11;words=[<Word index=11;text=l';lemma=le;upos=DET;xpos=_;feats=Definite=Def|Gender=Fem|Number=Sing|PronType=Art;governor=13;dependency_relation=det>]>\n",
      "<Token index=12;words=[<Word index=12;text=ancienne;lemma=ancien;upos=ADJ;xpos=_;feats=Gender=Fem|Number=Sing;governor=13;dependency_relation=amod>]>\n",
      "<Token index=13;words=[<Word index=13;text=bourgeoisie;lemma=bourgeoisie;upos=NOUN;xpos=_;feats=Gender=Fem|Number=Sing;governor=9;dependency_relation=nmod>]>\n",
      "<Token index=14;words=[<Word index=14;text=.;lemma=.;upos=PUNCT;xpos=_;feats=_;governor=3;dependency_relation=punct>]>\n"
     ]
    }
   ],
   "source": [
    "import stanfordnlp\n",
    "\n",
    "model_dir='/pi/ai/corenlp'\n",
    "config = {\n",
    "\t'processors': 'tokenize,mwt,pos,lemma,depparse', # Comma-separated list of processors to use\n",
    "\t'lang': 'fr', # Language code for the language to build the Pipeline in\n",
    "\t'tokenize_model_path': model_dir+'/fr_gsd_models/fr_gsd_tokenizer.pt', # Processor-specific arguments are set with keys \"{processor_name}_{argument_name}\"\n",
    "\t'mwt_model_path': model_dir+'/fr_gsd_models/fr_gsd_mwt_expander.pt',\n",
    "\t'pos_model_path': model_dir+'/fr_gsd_models/fr_gsd_tagger.pt',\n",
    "\t'pos_pretrain_path': model_dir+'/fr_gsd_models/fr_gsd.pretrain.pt',\n",
    "\t'lemma_model_path': model_dir+'/fr_gsd_models/fr_gsd_lemmatizer.pt',\n",
    "\t'depparse_model_path': model_dir+'/fr_gsd_models/fr_gsd_parser.pt',\n",
    "\t'depparse_pretrain_path': model_dir+'/fr_gsd_models/fr_gsd.pretrain.pt'\n",
    "}\n",
    "nlp = stanfordnlp.Pipeline(**config) # Initialize the pipeline using a configuration dict\n",
    "doc = nlp(\"Van Gogh grandit au sein d'une famille de l'ancienne bourgeoisie.\") # Run the pipeline on input text\n",
    "doc.sentences[0].print_tokens() # Look at the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-28T02:59:20.135830Z",
     "start_time": "2019-05-28T02:59:19.981911Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text: Je \tlemma: il\tupos: PRON\txpos: _\n",
      "text: suis \tlemma: être\tupos: AUX\txpos: _\n",
      "text: un \tlemma: un\tupos: DET\txpos: _\n",
      "text: étudiant \tlemma: étudiant\tupos: NOUN\txpos: _\n",
      "('Je', '4', 'nsubj')\n",
      "('suis', '4', 'cop')\n",
      "('un', '4', 'det')\n",
      "('étudiant', '0', 'root')\n"
     ]
    }
   ],
   "source": [
    "def analyse(sents):\n",
    "    doc = nlp(sents)\n",
    "    print(*[f'text: {word.text+\" \"}\\tlemma: {word.lemma}\\tupos: {word.upos}\\txpos: {word.xpos}' for sent in doc.sentences for word in sent.words], sep='\\n')\n",
    "    doc.sentences[0].print_dependencies()\n",
    "\n",
    "sents='Je suis un étudiant'\n",
    "analyse(sents)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
